{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12373553,"sourceType":"datasetVersion","datasetId":7801879}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport nibabel as nib\nimport os\nimport re\nimport xml.etree.ElementTree as ET\nfrom tqdm import tqdm\nfrom scipy.stats import entropy\nfrom skimage.exposure import histogram\nimport cv2\nfrom tensorflow.keras.utils import plot_model\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom torch.utils.data import Dataset, DataLoader\nimport gc\nfrom torch.cuda.amp import autocast, GradScaler\nscaler = GradScaler()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:22:40.146858Z","iopub.execute_input":"2025-07-16T19:22:40.147532Z","iopub.status.idle":"2025-07-16T19:22:40.154174Z","shell.execute_reply.started":"2025-07-16T19:22:40.147503Z","shell.execute_reply":"2025-07-16T19:22:40.153374Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/573148028.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nmetadata = '/kaggle/input/adni-processed-complete/ADNI1_Complete_1Yr_1.5T_6_20_2025.csv'\nmeta = pd.read_csv(metadata)\nmeta.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:22:40.155413Z","iopub.execute_input":"2025-07-16T19:22:40.155596Z","iopub.status.idle":"2025-07-16T19:22:40.301253Z","shell.execute_reply.started":"2025-07-16T19:22:40.155582Z","shell.execute_reply":"2025-07-16T19:22:40.300545Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"  Image Data ID     Subject Group Sex  Age Visit Modality  \\\n0       I112538  941_S_1311   MCI   M   70   m12      MRI   \n1        I97341  941_S_1311   MCI   M   70   m06      MRI   \n2        I97327  941_S_1311   MCI   M   69    sc      MRI   \n3        I75150  941_S_1202    CN   M   78   m06      MRI   \n4       I105437  941_S_1202    CN   M   79   m12      MRI   \n\n                                  Description       Type   Acq Date Format  \\\n0    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  6/01/2008  NiFTI   \n1  MPR-R; GradWarp; B1 Correction; N3; Scaled  Processed  9/27/2007  NiFTI   \n2    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  3/02/2007  NiFTI   \n3    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  8/24/2007  NiFTI   \n4    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  2/28/2008  NiFTI   \n\n   Downloaded  \n0         NaN  \n1         NaN  \n2         NaN  \n3         NaN  \n4         NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image Data ID</th>\n      <th>Subject</th>\n      <th>Group</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>Visit</th>\n      <th>Modality</th>\n      <th>Description</th>\n      <th>Type</th>\n      <th>Acq Date</th>\n      <th>Format</th>\n      <th>Downloaded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I112538</td>\n      <td>941_S_1311</td>\n      <td>MCI</td>\n      <td>M</td>\n      <td>70</td>\n      <td>m12</td>\n      <td>MRI</td>\n      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n      <td>Processed</td>\n      <td>6/01/2008</td>\n      <td>NiFTI</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I97341</td>\n      <td>941_S_1311</td>\n      <td>MCI</td>\n      <td>M</td>\n      <td>70</td>\n      <td>m06</td>\n      <td>MRI</td>\n      <td>MPR-R; GradWarp; B1 Correction; N3; Scaled</td>\n      <td>Processed</td>\n      <td>9/27/2007</td>\n      <td>NiFTI</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I97327</td>\n      <td>941_S_1311</td>\n      <td>MCI</td>\n      <td>M</td>\n      <td>69</td>\n      <td>sc</td>\n      <td>MRI</td>\n      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n      <td>Processed</td>\n      <td>3/02/2007</td>\n      <td>NiFTI</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I75150</td>\n      <td>941_S_1202</td>\n      <td>CN</td>\n      <td>M</td>\n      <td>78</td>\n      <td>m06</td>\n      <td>MRI</td>\n      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n      <td>Processed</td>\n      <td>8/24/2007</td>\n      <td>NiFTI</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I105437</td>\n      <td>941_S_1202</td>\n      <td>CN</td>\n      <td>M</td>\n      <td>79</td>\n      <td>m12</td>\n      <td>MRI</td>\n      <td>MPR; GradWarp; B1 Correction; N3; Scaled</td>\n      <td>Processed</td>\n      <td>2/28/2008</td>\n      <td>NiFTI</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"matching_index = meta[(meta['Image Data ID'] == 'I112538') & (meta['Subject'] == '941_S_1311')].index\nmatching_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:22:40.301989Z","iopub.execute_input":"2025-07-16T19:22:40.302245Z","iopub.status.idle":"2025-07-16T19:22:40.323128Z","shell.execute_reply.started":"2025-07-16T19:22:40.302223Z","shell.execute_reply":"2025-07-16T19:22:40.322449Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Index([0], dtype='int64')"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"meta.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:22:40.324560Z","iopub.execute_input":"2025-07-16T19:22:40.324748Z","iopub.status.idle":"2025-07-16T19:22:40.343270Z","shell.execute_reply.started":"2025-07-16T19:22:40.324734Z","shell.execute_reply":"2025-07-16T19:22:40.342655Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"(2294, 12)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"import os\n\nbase_path = \"/kaggle/input/adni-processed-complete/ADNI_Processed/ADNI_Processed\"\nsubjects = []\nfor folder in os.listdir(base_path):\n    print(folder)\n    for subject in os.listdir(os.path.join(base_path, folder)):\n        subjects.append(subject)\n\nsubjects = list(set(subjects))\nl_s = len(subjects)\nprint(\"Total Subjects\", l_s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:22:40.343844Z","iopub.execute_input":"2025-07-16T19:22:40.343996Z","iopub.status.idle":"2025-07-16T19:22:40.420414Z","shell.execute_reply.started":"2025-07-16T19:22:40.343984Z","shell.execute_reply":"2025-07-16T19:22:40.419714Z"}},"outputs":[{"name":"stdout","text":"ADNI1_Complete 1Yr 1.5T8\nADNI1_Complete 1Yr 1.5T1\nADNI1_Complete 1Yr 1.5T6\nADNI1_Complete 1Yr 1.5T4\nADNI1_Complete 1Yr 1.5T9\nADNI1_Complete 1Yr 1.5T3\nADNI1_Complete 1Yr 1.5T5\nADNI1_Complete 1Yr 1.5T2\nADNI1_Complete 1Yr 1.5T\nADNI1_Complete 1Yr 1.5T7\nTotal Subjects 639\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import random\n\nrandom.shuffle(subjects)\ntrain_subjects = subjects[:int(0.7*l_s)]\nval_subjects = subjects[int(0.7*l_s):int(0.8*l_s)]\ntest_subjects = subjects[int(0.8*l_s):]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:22:40.421172Z","iopub.execute_input":"2025-07-16T19:22:40.421491Z","iopub.status.idle":"2025-07-16T19:22:40.425551Z","shell.execute_reply.started":"2025-07-16T19:22:40.421469Z","shell.execute_reply":"2025-07-16T19:22:40.424791Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"train_file_paths = []\nval_file_paths = []\ntest_file_paths = []\n\ntrain_targets = []\nval_targets = []\ntest_targets = []\n\nfor folder in os.listdir(base_path):\n    print(folder)\n    for subject in os.listdir(os.path.join(base_path, folder)):\n        for image in os.listdir(os.path.join(base_path, folder, subject)):\n            for file in os.listdir(os.path.join(base_path, folder, subject, image)):\n                file_path = os.path.join(base_path, folder, subject, image, file)\n                target_idx = meta[((meta['Subject'] == subject) & (meta['Image Data ID'] == image))].index\n                label_str = str(meta.iloc[target_idx[0], 2])\n                if label_str == 'MCI':\n                    continue\n                if subject in train_subjects:\n                    train_file_paths.append((label_str, file_path))\n                elif subject in val_subjects:\n                    val_file_paths.append((label_str, file_path))\n                elif subject in test_subjects:\n                    test_file_paths.append((label_str, file_path))\n                else:\n                    print(\"Error Subject not in any split\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:22:40.426214Z","iopub.execute_input":"2025-07-16T19:22:40.426497Z","iopub.status.idle":"2025-07-16T19:22:53.571819Z","shell.execute_reply.started":"2025-07-16T19:22:40.426472Z","shell.execute_reply":"2025-07-16T19:22:53.571275Z"}},"outputs":[{"name":"stdout","text":"ADNI1_Complete 1Yr 1.5T8\nADNI1_Complete 1Yr 1.5T1\nADNI1_Complete 1Yr 1.5T6\nADNI1_Complete 1Yr 1.5T4\nADNI1_Complete 1Yr 1.5T9\nADNI1_Complete 1Yr 1.5T3\nADNI1_Complete 1Yr 1.5T5\nADNI1_Complete 1Yr 1.5T2\nADNI1_Complete 1Yr 1.5T\nADNI1_Complete 1Yr 1.5T7\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"print(len(train_file_paths))\nprint(len(val_file_paths))\nprint(len(test_file_paths))\n# file_paths -> contains paths of all files\n# targets -> corresponding targets (in the same order as file_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:22:53.572433Z","iopub.execute_input":"2025-07-16T19:22:53.572612Z","iopub.status.idle":"2025-07-16T19:22:53.576672Z","shell.execute_reply.started":"2025-07-16T19:22:53.572591Z","shell.execute_reply":"2025-07-16T19:22:53.576122Z"}},"outputs":[{"name":"stdout","text":"842\n122\n217\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"def center_crop(image, crop_size=128):\n    h, w = image.shape\n    if h < crop_size or w < crop_size:\n        return None\n    top = (h - crop_size) // 2\n    left = (w - crop_size) // 2\n    return image[top:top+crop_size, left:left+crop_size]\n\ndef image_entropy(img):\n    hist, _ = histogram(img)\n    hist = hist / np.sum(hist)\n    return entropy(hist, base=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:22:53.577295Z","iopub.execute_input":"2025-07-16T19:22:53.577454Z","iopub.status.idle":"2025-07-16T19:22:53.603953Z","shell.execute_reply.started":"2025-07-16T19:22:53.577441Z","shell.execute_reply":"2025-07-16T19:22:53.603426Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class EntropySlicesDataset(Dataset):\n    def __init__(self, actual_paths, N=100, crop_size=128, transform=None):\n        \"\"\"\n        Args:\n            actual_paths: list of tuples (label_str, path_to_nifti)\n            N: number of top entropy slices per subject\n            crop_size: center crop size\n            transform: optional torchvision transforms on the final subject_volume tensor\n        \"\"\"\n        self.actual_paths = actual_paths\n        self.N = N\n        self.crop_size = crop_size\n        self.transform = transform\n        self.samples = self._filter_valid_subjects()\n\n    def _filter_valid_subjects(self):\n        \"\"\"\n        Pre-screen subjects to exclude those with fewer than N slices.\n        Returns a list of (label_val, path) tuples that have enough slices.\n        \"\"\"\n        valid = []\n        print(\"Pre-scanning subjects to exclude insufficient-slice cases...\")\n        for label, path in tqdm(self.actual_paths):\n            scan = nib.load(path)\n            data = scan.get_fdata()\n            slice_info = []\n\n            for axis in [0, 1, 2]:\n                for i in range(data.shape[axis]):\n                    if axis == 0:\n                        slice_ = data[i, :, :]\n                    elif axis == 1:\n                        slice_ = data[:, i, :]\n                    else:\n                        slice_ = data[:, :, i]\n\n                    cropped = center_crop(slice_, crop_size=self.crop_size)\n                    if cropped is None:\n                        continue\n\n                    ent = image_entropy(cropped)\n                    slice_info.append((ent,))  # Only entropy needed for counting\n\n            if len(slice_info) >= self.N:\n                valid.append((label, path))\n            else:\n                print(f\"[!] Skipped subject at {path}: only {len(slice_info)} valid slices\")\n        return valid\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        label_str, path = self.samples[idx]\n        scan = nib.load(path)\n        data = scan.get_fdata()\n        label_val = 0 if label_str == 'AD' else 1\n\n        slice_info = []\n        for axis in [0, 1, 2]:\n            for i in range(data.shape[axis]):\n                if axis == 0:\n                    slice_ = data[i, :, :]\n                elif axis == 1:\n                    slice_ = data[:, i, :]\n                else:\n                    slice_ = data[:, :, i]\n\n                cropped = center_crop(slice_, crop_size=self.crop_size)\n                if cropped is None:\n                    continue\n\n                ent = image_entropy(cropped)\n                slice_info.append((ent, cropped))\n\n        slice_info.sort(reverse=True, key=lambda x: x[0])\n        top_slices = slice_info[:self.N]\n\n        subject_slices = []\n        for _, slice_2d in top_slices:\n            slice_2d = slice_2d[..., np.newaxis]                   # (128,128,1)\n            slice_rgb = np.repeat(slice_2d, 3, axis=-1)            # (128,128,3)\n            slice_rgb_chw = np.transpose(slice_rgb, (2, 0, 1))     # (3,128,128)\n            subject_slices.append(slice_rgb_chw)\n\n        subject_volume = np.stack(subject_slices, axis=0)          # (N,3,128,128)\n\n        subject_volume = torch.from_numpy(subject_volume).float()  # convert to torch tensor\n\n        if self.transform:\n            subject_volume = self.transform(subject_volume)\n\n        return subject_volume, label_val","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:22:53.606211Z","iopub.execute_input":"2025-07-16T19:22:53.606416Z","iopub.status.idle":"2025-07-16T19:22:53.622582Z","shell.execute_reply.started":"2025-07-16T19:22:53.606401Z","shell.execute_reply":"2025-07-16T19:22:53.622064Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"train_dataset = EntropySlicesDataset(train_file_paths, N=100, crop_size=128)\nval_dataset   = EntropySlicesDataset(val_file_paths, N=100, crop_size=128)\ntest_dataset  = EntropySlicesDataset(test_file_paths, N=100, crop_size=128)\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:22:53.623216Z","iopub.execute_input":"2025-07-16T19:22:53.623749Z","iopub.status.idle":"2025-07-16T19:39:34.333540Z","shell.execute_reply.started":"2025-07-16T19:22:53.623730Z","shell.execute_reply":"2025-07-16T19:39:34.332940Z"}},"outputs":[{"name":"stdout","text":"Pre-scanning subjects to exclude insufficient-slice cases...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 842/842 [11:33<00:00,  1.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pre-scanning subjects to exclude insufficient-slice cases...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 122/122 [01:48<00:00,  1.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pre-scanning subjects to exclude insufficient-slice cases...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 217/217 [03:18<00:00,  1.10it/s]\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\nclass ResNetPerSlice(nn.Module):\n    def __init__(self, dropout_p=0.3):\n        super().__init__()\n        resnet = models.resnet152(weights='IMAGENET1K_V1')\n        modules = list(resnet.children())[:-2]  # keep feature maps before final pool\n        self.features = nn.Sequential(*modules)\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.flatten = nn.Flatten()\n        self.dropout = nn.Dropout2d(p=dropout_p)  # spatial dropout\n\n    def forward(self, x):\n        x = self.features(x)         # (B, 2048, H', W')\n        x = self.dropout(x)\n        x = self.pool(x)             # (B, 2048, 1, 1)\n        x = self.flatten(x)          # (B, 2048)\n        return x\n\nclass AttentionXAIClassifier(nn.Module):\n    def __init__(self, feature_dim=2048, attn_hidden=128, num_classes=1, dropout_p=0.5):\n        super().__init__()\n        self.cnn_per_slice = ResNetPerSlice(dropout_p=0.3)\n\n        # Attention module → use LayerNorm instead of BatchNorm1d\n        self.attn_fc = nn.Sequential(\n            nn.Linear(feature_dim, attn_hidden, bias=False),\n            nn.LayerNorm(attn_hidden),     # ← swapped\n            nn.Tanh(),\n            nn.Dropout(p=dropout_p),\n            nn.Linear(attn_hidden, 1)\n        )\n\n        # Classifier head → also use LayerNorm\n        self.classifier_fc = nn.Sequential(\n            nn.Linear(feature_dim, 128, bias=False),\n            nn.LayerNorm(128),             # ← swapped\n            nn.ReLU(),\n            nn.Dropout(p=dropout_p),\n            nn.Linear(128, num_classes)    # num_classes=1 for BCEWithLogits\n        )\n\n    def forward(self, x, noise_p=0.01):\n        B, N, C, H, W = x.shape\n\n        # optional: inject tiny gaussian noise to inputs\n        if self.training and noise_p>0:\n            x = x + torch.randn_like(x) * noise_p\n\n        x = x.view(B * N, C, H, W)\n        features = self.cnn_per_slice(x)         # (B*N, 2048)\n        features = features.view(B, N, -1)       # (B, N, 2048)\n\n        # compute attention scores\n        w = self.attn_fc(features.view(-1, features.size(-1)))  # (B*N, 1)\n        w = w.view(B, N)                                        # (B, N)\n        alpha = F.softmax(w, dim=1).unsqueeze(-1)                # (B, N, 1)\n\n        # weighted pooling\n        pooled = torch.sum(alpha * features, dim=1)             # (B, 2048)\n        logits = self.classifier_fc(pooled)                    # (B, num_classes)\n\n        return logits, alpha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:39:34.334347Z","iopub.execute_input":"2025-07-16T19:39:34.334548Z","iopub.status.idle":"2025-07-16T19:39:34.350489Z","shell.execute_reply.started":"2025-07-16T19:39:34.334533Z","shell.execute_reply":"2025-07-16T19:39:34.349927Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def train(model, train_loader, val_loader, device, num_epochs=10, lr=1e-4):\n    # — compute class imbalance from train_loader —\n    all_labels = []\n    for _, y in train_loader:\n        all_labels.append(y)\n    all_labels = torch.cat(all_labels).to(device)\n    neg_count = (all_labels == 0).sum().float()\n    pos_count = (all_labels == 1).sum().float()\n    pos_weight = torch.tensor([neg_count / pos_count], device=device)\n\n    # 1) Optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n    \n    # 2) LR scheduler (reduce on plateau of val_loss) — drop verbose=True\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer,\n        mode='min',\n        factor=0.5,\n        patience=2,\n        min_lr=1e-6\n    )\n    \n    # 3) Criterion (with computed pos_weight)\n    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n    \n    # 4) Label‑smoothing factor\n    eps = 0.1\n\n    scaler = GradScaler()\n    model.to(device)\n\n    for epoch in range(1, num_epochs + 1):\n        # ----------------\n        # Training phase\n        # ----------------\n        model.train()\n        train_loss, train_correct, total = 0.0, 0, 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\")\n\n        for X, y in pbar:\n            X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            optimizer.zero_grad()\n\n            with autocast():\n                outputs, _ = model(X)\n                outputs = outputs.squeeze(1)\n                # manual label smoothing\n                targets_s = y.float() * (1 - eps) + 0.5 * eps\n                loss = criterion(outputs, targets_s)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            train_loss    += loss.item() * X.size(0)\n            probs          = torch.sigmoid(outputs)\n            preds          = (probs > 0.5).long()\n            train_correct += (preds == y.long()).sum().item()\n            total         += y.size(0)\n\n            pbar.set_postfix(loss=loss.item(), acc=100 * train_correct / total)\n            del outputs, loss, probs, preds, targets_s\n\n        avg_train_loss = train_loss / total\n        train_acc      = 100 * train_correct / total\n\n        # ------------------\n        # Validation phase\n        # ------------------\n        model.eval()\n        val_loss, val_correct, total_val = 0.0, 0, 0\n\n        with torch.no_grad(), autocast():\n            for X_val, y_val in val_loader:\n                X_val, y_val = X_val.to(device, non_blocking=True), y_val.to(device, non_blocking=True)\n                outputs, _ = model(X_val)\n                outputs = outputs.squeeze(1)\n                targets_s = y_val.float() * (1 - eps) + 0.5 * eps\n                loss = criterion(outputs, targets_s)\n\n                val_loss    += loss.item() * X_val.size(0)\n                probs        = torch.sigmoid(outputs)\n                preds        = (probs > 0.5).long()\n                val_correct += (preds == y_val.long()).sum().item()\n                total_val   += y_val.size(0)\n\n                del outputs, loss, probs, preds, targets_s\n\n        avg_val_loss = val_loss / total_val\n        val_acc      = 100 * val_correct / total_val\n\n        print(f\"Epoch {epoch:02d}: \"\n              f\"Train Loss {avg_train_loss:.4f} Acc {train_acc:.2f}%, \"\n              f\"Val Loss {avg_val_loss:.4f} Acc {val_acc:.2f}%\")\n\n        # — Step the LR scheduler on validation loss (no verbose flag) —\n        scheduler.step(avg_val_loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:40:59.845752Z","iopub.execute_input":"2025-07-16T19:40:59.846353Z","iopub.status.idle":"2025-07-16T19:40:59.857757Z","shell.execute_reply.started":"2025-07-16T19:40:59.846328Z","shell.execute_reply":"2025-07-16T19:40:59.857048Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"model = AttentionXAIClassifier(num_classes=1) # adjust num_classes if binary\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n    model = torch.nn.DataParallel(model)\n\nmodel.to(device) \n\ntrain(model, train_loader, test_loader, device, num_epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:41:00.136372Z","iopub.execute_input":"2025-07-16T19:41:00.136575Z","iopub.status.idle":"2025-07-16T20:55:34.148650Z","shell.execute_reply.started":"2025-07-16T19:41:00.136558Z","shell.execute_reply":"2025-07-16T20:55:34.147712Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3127691956.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\nEpoch 1/10 [Train]:   0%|          | 0/211 [00:00<?, ?it/s]/tmp/ipykernel_36/3127691956.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 1/10 [Train]: 100%|██████████| 211/211 [05:37<00:00,  1.60s/it, acc=62.4, loss=0.444]\n/tmp/ipykernel_36/3127691956.py:73: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.no_grad(), autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01: Train Loss 0.5220 Acc 62.35%, Val Loss 0.7238 Acc 34.56%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10 [Train]: 100%|██████████| 211/211 [05:35<00:00,  1.59s/it, acc=77.1, loss=0.303]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02: Train Loss 0.4472 Acc 77.08%, Val Loss 0.3726 Acc 82.95%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10 [Train]: 100%|██████████| 211/211 [05:32<00:00,  1.58s/it, acc=84.6, loss=0.245]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03: Train Loss 0.3883 Acc 84.56%, Val Loss 0.6847 Acc 57.60%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10 [Train]: 100%|██████████| 211/211 [05:26<00:00,  1.55s/it, acc=91.8, loss=0.27] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 04: Train Loss 0.2952 Acc 91.81%, Val Loss 0.3353 Acc 88.02%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10 [Train]: 100%|██████████| 211/211 [05:33<00:00,  1.58s/it, acc=95.1, loss=0.165]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05: Train Loss 0.2439 Acc 95.13%, Val Loss 0.3527 Acc 85.25%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10 [Train]: 100%|██████████| 211/211 [05:32<00:00,  1.57s/it, acc=96.8, loss=0.182]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06: Train Loss 0.2230 Acc 96.79%, Val Loss 0.7432 Acc 57.14%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10 [Train]: 100%|██████████| 211/211 [05:30<00:00,  1.57s/it, acc=98.6, loss=0.163]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07: Train Loss 0.1976 Acc 98.57%, Val Loss 0.4608 Acc 76.96%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10 [Train]: 100%|██████████| 211/211 [05:26<00:00,  1.55s/it, acc=99.2, loss=0.18] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 08: Train Loss 0.1859 Acc 99.17%, Val Loss 0.4211 Acc 82.95%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10 [Train]: 100%|██████████| 211/211 [05:32<00:00,  1.58s/it, acc=99.3, loss=0.165]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 09: Train Loss 0.1855 Acc 99.29%, Val Loss 0.3683 Acc 84.33%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10 [Train]: 100%|██████████| 211/211 [05:35<00:00,  1.59s/it, acc=100, loss=0.187]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss 0.1753 Acc 100.00%, Val Loss 0.3856 Acc 84.79%\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"torch.save(model.state_dict(), \"axial_weights.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T20:55:57.160364Z","iopub.execute_input":"2025-07-16T20:55:57.160933Z","iopub.status.idle":"2025-07-16T20:55:57.601717Z","shell.execute_reply.started":"2025-07-16T20:55:57.160906Z","shell.execute_reply":"2025-07-16T20:55:57.601127Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"model.eval()  # set model to evaluation mode\nall_preds = []\nall_probs = []\nall_labels = []\n\nwith torch.no_grad():\n    for X, y in test_loader:\n        X, y = X.to(device), y.to(device)\n        outputs, _ = model(X)          # outputs: logits of shape (B,1)\n        outputs = outputs.squeeze(1)   # (B,)\n\n        probs = torch.sigmoid(outputs)   # get probabilities\n        preds = (probs > 0.5).long()     # binary thresholding\n\n        all_preds.append(preds.cpu())\n        all_probs.append(probs.cpu())\n        all_labels.append(y.cpu())\n\n# Concatenate all batches\nall_preds = torch.cat(all_preds).numpy()\nall_probs = torch.cat(all_probs).numpy()\nall_labels = torch.cat(all_labels).numpy()\n\n# Create classification report\nprint(classification_report(all_labels, all_preds, target_names=[\"AD\", \"CN\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T21:02:40.531559Z","iopub.execute_input":"2025-07-16T21:02:40.532057Z","iopub.status.idle":"2025-07-16T21:03:59.645896Z","shell.execute_reply.started":"2025-07-16T21:02:40.532031Z","shell.execute_reply":"2025-07-16T21:03:59.645131Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n          AD       0.72      0.91      0.80        75\n          CN       0.94      0.82      0.88       142\n\n    accuracy                           0.85       217\n   macro avg       0.83      0.86      0.84       217\nweighted avg       0.87      0.85      0.85       217\n\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\n# Create the confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\n\n# Optional: normalize='true' for percentage matrix\n# cm = confusion_matrix(all_labels, all_preds, normalize='true')\n\n# Display the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"AD\", \"CN\"])\ndisp.plot(cmap=\"Blues\", values_format='d')  # use '.2f' for float % if normalized\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T21:03:59.647378Z","iopub.execute_input":"2025-07-16T21:03:59.647614Z","iopub.status.idle":"2025-07-16T21:03:59.798926Z","shell.execute_reply.started":"2025-07-16T21:03:59.647593Z","shell.execute_reply":"2025-07-16T21:03:59.798359Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"acc = (all_preds == all_labels).mean()\nprint(f\"\\nOverall Test Accuracy: {acc*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T20:57:17.967470Z","iopub.execute_input":"2025-07-16T20:57:17.967936Z","iopub.status.idle":"2025-07-16T20:57:17.971916Z","shell.execute_reply.started":"2025-07-16T20:57:17.967918Z","shell.execute_reply":"2025-07-16T20:57:17.971197Z"}},"outputs":[{"name":"stdout","text":"\nOverall Test Accuracy: 84.79%\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\ntrain_dataset = ScanDataset(X_train, y_train)\ntest_dataset  = ScanDataset(X_test, y_test)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:39:37.212227Z","iopub.status.idle":"2025-07-16T19:39:37.212465Z","shell.execute_reply.started":"2025-07-16T19:39:37.212352Z","shell.execute_reply":"2025-07-16T19:39:37.212364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2)\ntest_loader  = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=2)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:39:37.213408Z","iopub.status.idle":"2025-07-16T19:39:37.213657Z","shell.execute_reply.started":"2025-07-16T19:39:37.213509Z","shell.execute_reply":"2025-07-16T19:39:37.213518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nclass TimeDistributedResNet(nn.Module):\n    def __init__(self, base_model):\n        super(TimeDistributedResNet, self).__init__()\n        self.base_model = base_model\n\n    def forward(self, x):\n        # x: (batch, time, 3, 128,128)\n        B, T, C, H, W = x.size()\n        x = x.view(B*T, C, H, W)\n        x = self.base_model(x)                   # (B*T, 2048)\n        x = x.view(B, T, -1)                     # (B, T, 2048)\n        return x\n\nclass ScanLSTMClassifier(nn.Module):\n    def __init__(self):\n        super(ScanLSTMClassifier, self).__init__()\n        base_resnet = models.resnet152(weights='IMAGENET1K_V1')\n        base_resnet.fc = nn.Identity()\n        self.resnet = TimeDistributedResNet(base_resnet)\n\n        for param in self.resnet.parameters():\n            param.requires_grad = False          # freeze ResNet\n\n        self.lstm = nn.LSTM(2048, 512, batch_first=True)\n        self.fc1 = nn.Linear(256, 128)\n        self.dropout = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(128, 1)\n\n    def forward(self, x):\n        x = self.resnet(x)                       # (B, T, 2048)\n        _, (hn, _) = self.lstm(x)                # hn: (1, B, 256)\n        x = hn.squeeze(0)                        # (B, 256)\n        x = F.relu(self.fc1(x))                  # (B, 128)\n        x = self.dropout(x)\n        x = torch.sigmoid(self.fc2(x)).squeeze(-1)  # (B,)\n        return x\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:39:37.214575Z","iopub.status.idle":"2025-07-16T19:39:37.214895Z","shell.execute_reply.started":"2025-07-16T19:39:37.214743Z","shell.execute_reply":"2025-07-16T19:39:37.214757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel1 = ScanLSTMClassifier().to(device)\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model1.parameters(), lr=1e-6)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:39:37.216956Z","iopub.status.idle":"2025-07-16T19:39:37.217265Z","shell.execute_reply.started":"2025-07-16T19:39:37.217116Z","shell.execute_reply":"2025-07-16T19:39:37.217129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nclass ScanDataset(Dataset):\n    def __init__(self, scans, labels):\n        self.scans = scans\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.scans)\n\n    def __getitem__(self, idx):\n        x = self.scans[idx]  # shape: (100, 3, 128, 128)\n        y = self.labels[idx]\n        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:39:37.218730Z","iopub.status.idle":"2025-07-16T19:39:37.218966Z","shell.execute_reply.started":"2025-07-16T19:39:37.218864Z","shell.execute_reply":"2025-07-16T19:39:37.218874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nN = 100  # number of entropy-based slices\ncrop_size = 128\n\nX, y = [], []\n\nfor label, path in tqdm(actual_paths):\n    scan = nib.load(path)\n    data = scan.get_fdata()\n    label_val = 0 if label == 'AD' else 1\n\n    slice_info = []\n\n    for axis in [0, 1, 2]:  \n        for i in range(data.shape[axis]):\n            # Extract 2D slice along the given axis \n            if axis == 0:\n                slice_ = data[i, :, :]\n            elif axis == 1:\n                slice_ = data[:, i, :]\n            else:\n                slice_ = data[:, :, i]\n\n            # Crop and skip empty slices\n            cropped = center_crop(slice_, crop_size=crop_size)\n            if cropped is None:\n                continue\n\n            # Compute entropy\n            ent = image_entropy(cropped)\n            slice_info.append((ent, cropped))\n\n    # Sort slices by entropy\n    slice_info.sort(reverse=True, key=lambda x: x[0])\n    top_slices = slice_info[:N]\n\n    if len(top_slices) < N:\n        print(f\"[!] Skipped subject: only {len(top_slices)} slices\")\n        continue\n\n    subject_slices = []\n\n    for _, slice_2d in top_slices:\n        # Grayscale slice is (128,128), add channel axis → (128,128,1)\n        slice_2d = slice_2d[..., np.newaxis]\n\n        # Repeat channel 3 times → (128,128,3)\n        slice_rgb = np.repeat(slice_2d, 3, axis=-1)\n\n        # Convert to channels-first → (3,128,128)\n        slice_rgb_chw = np.transpose(slice_rgb, (2,0,1))\n\n        subject_slices.append(slice_rgb_chw)\n\n    # Stack into (N,3,128,128)\n    subject_volume = np.stack(subject_slices, axis=0)\n\n    X.append(subject_volume)\n    y.append(label_val)\n\nX = np.stack(X)  # shape: (num_subjects, N, 128, 128, 1)\ny = np.array(y)  # shape: (num_subjects,)\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T19:39:37.219737Z","iopub.status.idle":"2025-07-16T19:39:37.219933Z","shell.execute_reply.started":"2025-07-16T19:39:37.219839Z","shell.execute_reply":"2025-07-16T19:39:37.219847Z"}},"outputs":[],"execution_count":null}]}