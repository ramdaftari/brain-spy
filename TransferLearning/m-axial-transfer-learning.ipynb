{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12373553,"sourceType":"datasetVersion","datasetId":7801879},{"sourceId":499979,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":397040,"modelId":415461}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport nibabel as nib\nimport os\nimport re\nimport xml.etree.ElementTree as ET\nfrom tqdm import tqdm\nfrom scipy.stats import entropy\nfrom skimage.exposure import histogram\nimport cv2\nfrom tensorflow.keras.utils import plot_model\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom torch.utils.data import Dataset, DataLoader\nimport gc\nfrom torch.cuda.amp import autocast, GradScaler\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-31T18:21:11.096257Z","iopub.execute_input":"2025-07-31T18:21:11.096806Z","iopub.status.idle":"2025-07-31T18:21:43.735215Z","shell.execute_reply.started":"2025-07-31T18:21:11.096769Z","shell.execute_reply":"2025-07-31T18:21:43.734594Z"}},"outputs":[{"name":"stderr","text":"2025-07-31 18:21:17.347616: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753986077.707123      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753986077.803579      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"metadata = '/kaggle/input/adni-processed-complete/ADNI1_Complete_1Yr_1.5T_6_20_2025.csv'\nmeta = pd.read_csv(metadata)\nprint(meta.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T18:21:43.736780Z","iopub.execute_input":"2025-07-31T18:21:43.737900Z","iopub.status.idle":"2025-07-31T18:21:43.778407Z","shell.execute_reply.started":"2025-07-31T18:21:43.737865Z","shell.execute_reply":"2025-07-31T18:21:43.777794Z"}},"outputs":[{"name":"stdout","text":"  Image Data ID     Subject Group Sex  Age Visit Modality  \\\n0       I112538  941_S_1311   MCI   M   70   m12      MRI   \n1        I97341  941_S_1311   MCI   M   70   m06      MRI   \n2        I97327  941_S_1311   MCI   M   69    sc      MRI   \n3        I75150  941_S_1202    CN   M   78   m06      MRI   \n4       I105437  941_S_1202    CN   M   79   m12      MRI   \n\n                                  Description       Type   Acq Date Format  \\\n0    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  6/01/2008  NiFTI   \n1  MPR-R; GradWarp; B1 Correction; N3; Scaled  Processed  9/27/2007  NiFTI   \n2    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  3/02/2007  NiFTI   \n3    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  8/24/2007  NiFTI   \n4    MPR; GradWarp; B1 Correction; N3; Scaled  Processed  2/28/2008  NiFTI   \n\n   Downloaded  \n0         NaN  \n1         NaN  \n2         NaN  \n3         NaN  \n4         NaN  \n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"matching_index = meta[(meta['Image Data ID'] == 'I112538') & (meta['Subject'] == '941_S_1311')].index\nmatching_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T18:21:43.779014Z","iopub.execute_input":"2025-07-31T18:21:43.779483Z","iopub.status.idle":"2025-07-31T18:21:43.796449Z","shell.execute_reply.started":"2025-07-31T18:21:43.779456Z","shell.execute_reply":"2025-07-31T18:21:43.795894Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Index([0], dtype='int64')"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"meta.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T18:21:43.797082Z","iopub.execute_input":"2025-07-31T18:21:43.797303Z","iopub.status.idle":"2025-07-31T18:21:43.815188Z","shell.execute_reply.started":"2025-07-31T18:21:43.797286Z","shell.execute_reply":"2025-07-31T18:21:43.814565Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(2294, 12)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"base_path = \"/kaggle/input/adni-processed-complete/ADNI_Processed/ADNI_Processed\"\nsubjects = []\nfor folder in os.listdir(base_path):\n    for subject in os.listdir(os.path.join(base_path, folder)):\n        subjects.append(subject)\n\nsubjects = list(set(subjects))\nl_s = len(subjects)\nprint(f\"Total Subjects: {l_s}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T18:21:43.816658Z","iopub.execute_input":"2025-07-31T18:21:43.816827Z","iopub.status.idle":"2025-07-31T18:21:43.928070Z","shell.execute_reply.started":"2025-07-31T18:21:43.816814Z","shell.execute_reply":"2025-07-31T18:21:43.927465Z"}},"outputs":[{"name":"stdout","text":"Total Subjects: 639\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import random\nrandom.shuffle(subjects)\ntrain_subjects = subjects[:int(0.7*l_s)]\nval_subjects = subjects[int(0.7*l_s):int(0.8*l_s)]\ntest_subjects = subjects[int(0.8*l_s):]\n\nprint(f\"Train subjects: {len(train_subjects)}\")\nprint(f\"Validation subjects: {len(val_subjects)}\")\nprint(f\"Test subjects: {len(test_subjects)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T18:21:43.928769Z","iopub.execute_input":"2025-07-31T18:21:43.929009Z","iopub.status.idle":"2025-07-31T18:21:43.933974Z","shell.execute_reply.started":"2025-07-31T18:21:43.928983Z","shell.execute_reply":"2025-07-31T18:21:43.933336Z"}},"outputs":[{"name":"stdout","text":"Train subjects: 447\nValidation subjects: 64\nTest subjects: 128\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"train_file_paths = []\nval_file_paths = []\ntest_file_paths = []\n\nfor folder in os.listdir(base_path):\n    print(f\"Processing folder: {folder}\")\n    for subject in os.listdir(os.path.join(base_path, folder)):\n        for image in os.listdir(os.path.join(base_path, folder, subject)):\n            for file in os.listdir(os.path.join(base_path, folder, subject, image)):\n                file_path = os.path.join(base_path, folder, subject, image, file)\n                \n                # Find corresponding metadata\n                target_idx = meta[((meta['Subject'] == subject) & (meta['Image Data ID'] == image))].index\n                \n                # Skip if no metadata found for the image\n                if target_idx.empty:\n                    continue\n                \n                label_str = str(meta.iloc[target_idx[0], 2])\n                \n                # MODIFICATION: Include all three classes (AD, CN, MCI)\n                # The original code skipped 'MCI'. This has been removed.\n                \n                if subject in train_subjects:\n                    train_file_paths.append((label_str, file_path))\n                elif subject in val_subjects:\n                    val_file_paths.append((label_str, file_path))\n                elif subject in test_subjects:\n                    test_file_paths.append((label_str, file_path))\n                else:\n                    # This case should ideally not happen with the current logic\n                    print(\"Error: Subject not in any split\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T18:21:43.934666Z","iopub.execute_input":"2025-07-31T18:21:43.934927Z","iopub.status.idle":"2025-07-31T18:22:01.312399Z","shell.execute_reply.started":"2025-07-31T18:21:43.934904Z","shell.execute_reply":"2025-07-31T18:22:01.311781Z"}},"outputs":[{"name":"stdout","text":"Processing folder: ADNI1_Complete 1Yr 1.5T8\nProcessing folder: ADNI1_Complete 1Yr 1.5T1\nProcessing folder: ADNI1_Complete 1Yr 1.5T6\nProcessing folder: ADNI1_Complete 1Yr 1.5T4\nProcessing folder: ADNI1_Complete 1Yr 1.5T9\nProcessing folder: ADNI1_Complete 1Yr 1.5T3\nProcessing folder: ADNI1_Complete 1Yr 1.5T5\nProcessing folder: ADNI1_Complete 1Yr 1.5T2\nProcessing folder: ADNI1_Complete 1Yr 1.5T\nProcessing folder: ADNI1_Complete 1Yr 1.5T7\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(f\"Number of training files: {len(train_file_paths)}\")\nprint(f\"Number of validation files: {len(val_file_paths)}\")\nprint(f\"Number of testing files: {len(test_file_paths)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T18:22:01.313278Z","iopub.execute_input":"2025-07-31T18:22:01.313466Z","iopub.status.idle":"2025-07-31T18:22:01.317724Z","shell.execute_reply.started":"2025-07-31T18:22:01.313450Z","shell.execute_reply":"2025-07-31T18:22:01.316911Z"}},"outputs":[{"name":"stdout","text":"Number of training files: 1619\nNumber of validation files: 231\nNumber of testing files: 444\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def center_crop(image, crop_size=128):\n    \"\"\"Crops the center of an image.\"\"\"\n    h, w = image.shape\n    if h < crop_size or w < crop_size:\n        return None\n    top = (h - crop_size) // 2\n    left = (w - crop_size) // 2\n    return image[top:top+crop_size, left:left+crop_size]\n\ndef image_entropy(img):\n    \"\"\"Calculates the entropy of an image.\"\"\"\n    hist, _ = histogram(img)\n    hist = hist / np.sum(hist)\n    return entropy(hist, base=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T18:22:01.318592Z","iopub.execute_input":"2025-07-31T18:22:01.319413Z","iopub.status.idle":"2025-07-31T18:22:01.359388Z","shell.execute_reply.started":"2025-07-31T18:22:01.319389Z","shell.execute_reply":"2025-07-31T18:22:01.358606Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class EntropySlicesDataset(Dataset):\n    def __init__(self, actual_paths, N=100, crop_size=128, transform=None):\n        \"\"\"\n        Args:\n            actual_paths: list of tuples (label_str, path_to_nifti)\n            N: number of top entropy slices per subject\n            crop_size: center crop size\n            transform: optional torchvision transforms\n        \"\"\"\n        self.actual_paths = actual_paths\n        self.N = N\n        self.crop_size = crop_size\n        self.transform = transform\n        \n        # MODIFICATION: Create a mapping for three classes\n        self.label_map = {'AD': 0, 'CN': 1, 'MCI': 2}\n        \n        self.samples = self._filter_valid_subjects()\n\n    def _filter_valid_subjects(self):\n        \"\"\"Pre-screens subjects to ensure they have enough valid slices.\"\"\"\n        valid = []\n        print(\"Pre-scanning subjects to exclude insufficient-slice cases...\")\n        for label, path in tqdm(self.actual_paths):\n            try:\n                scan = nib.load(path)\n                data = scan.get_fdata()\n                slice_count = 0\n\n                for axis in [0, 1, 2]:\n                    for i in range(data.shape[axis]):\n                        if axis == 0: slice_ = data[i, :, :]\n                        elif axis == 1: slice_ = data[:, i, :]\n                        else: slice_ = data[:, :, i]\n\n                        if center_crop(slice_, crop_size=self.crop_size) is not None:\n                            slice_count += 1\n                \n                if slice_count >= self.N:\n                    valid.append((label, path))\n                else:\n                    print(f\"[!] Skipped subject at {path}: only {slice_count} valid slices found, need {self.N}\")\n            except Exception as e:\n                print(f\"Error processing {path}: {e}\")\n        return valid\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        label_str, path = self.samples[idx]\n        scan = nib.load(path)\n        data = scan.get_fdata()\n        \n        # MODIFICATION: Use the label map for 3 classes\n        label_val = self.label_map[label_str]\n\n        slice_info = []\n        for axis in [0, 1, 2]:\n            for i in range(data.shape[axis]):\n                if axis == 0: slice_ = data[i, :, :]\n                elif axis == 1: slice_ = data[:, i, :]\n                else: slice_ = data[:, :, i]\n\n                cropped = center_crop(slice_, crop_size=self.crop_size)\n                if cropped is None:\n                    continue\n\n                ent = image_entropy(cropped)\n                slice_info.append((ent, cropped))\n\n        slice_info.sort(reverse=True, key=lambda x: x[0])\n        top_slices = slice_info[:self.N]\n\n        subject_slices = []\n        for _, slice_2d in top_slices:\n            slice_2d = slice_2d[..., np.newaxis]\n            slice_rgb = np.repeat(slice_2d, 3, axis=-1)\n            slice_rgb_chw = np.transpose(slice_rgb, (2, 0, 1))\n            subject_slices.append(slice_rgb_chw)\n\n        subject_volume = np.stack(subject_slices, axis=0)\n        subject_volume = torch.from_numpy(subject_volume).float()\n\n        if self.transform:\n            subject_volume = self.transform(subject_volume)\n\n        return subject_volume, label_val","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T18:22:01.360171Z","iopub.execute_input":"2025-07-31T18:22:01.360370Z","iopub.status.idle":"2025-07-31T18:22:01.386166Z","shell.execute_reply.started":"2025-07-31T18:22:01.360355Z","shell.execute_reply":"2025-07-31T18:22:01.385622Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_dataset = EntropySlicesDataset(train_file_paths, N=100, crop_size=128)\nval_dataset   = EntropySlicesDataset(val_file_paths, N=100, crop_size=128)\ntest_dataset  = EntropySlicesDataset(test_file_paths, N=100, crop_size=128)\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T18:22:01.386755Z","iopub.execute_input":"2025-07-31T18:22:01.386968Z","iopub.status.idle":"2025-07-31T18:33:15.390917Z","shell.execute_reply.started":"2025-07-31T18:22:01.386953Z","shell.execute_reply":"2025-07-31T18:33:15.390201Z"}},"outputs":[{"name":"stdout","text":"Pre-scanning subjects to exclude insufficient-slice cases...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1619/1619 [07:54<00:00,  3.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pre-scanning subjects to exclude insufficient-slice cases...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 231/231 [01:09<00:00,  3.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pre-scanning subjects to exclude insufficient-slice cases...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 444/444 [02:09<00:00,  3.43it/s]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"class ResNetPerSlice(nn.Module):\n    \"\"\"Feature extractor for a single 2D slice.\"\"\"\n    def __init__(self, dropout_p=0.3):\n        super().__init__()\n        resnet = models.resnet152(weights='IMAGENET1K_V1')\n        modules = list(resnet.children())[:-2]\n        self.features = nn.Sequential(*modules)\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.flatten = nn.Flatten()\n        self.dropout = nn.Dropout2d(p=dropout_p)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.dropout(x)\n        x = self.pool(x)\n        x = self.flatten(x)\n        return x\n\nclass AttentionXAIClassifier(nn.Module):\n    \"\"\"Full classifier model with attention mechanism.\"\"\"\n    def __init__(self, feature_dim=2048, attn_hidden=128, num_classes=3, dropout_p=0.5):\n        super().__init__()\n        self.cnn_per_slice = ResNetPerSlice(dropout_p=0.3)\n\n        self.attn_fc = nn.Sequential(\n            nn.Linear(feature_dim, attn_hidden, bias=False),\n            nn.LayerNorm(attn_hidden),\n            nn.Tanh(),\n            nn.Dropout(p=dropout_p),\n            nn.Linear(attn_hidden, 1)\n        )\n\n        # MODIFICATION: Classifier head adapted for `num_classes`\n        self.classifier_fc = nn.Sequential(\n            nn.Linear(feature_dim, 128, bias=False),\n            nn.LayerNorm(128),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_p),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x, noise_p=0.01):\n        B, N, C, H, W = x.shape\n\n        if self.training and noise_p > 0:\n            x = x + torch.randn_like(x) * noise_p\n\n        x = x.view(B * N, C, H, W)\n        features = self.cnn_per_slice(x)\n        features = features.view(B, N, -1)\n\n        w = self.attn_fc(features.view(-1, features.size(-1)))\n        w = w.view(B, N)\n        alpha = F.softmax(w, dim=1).unsqueeze(-1)\n\n        pooled = torch.sum(alpha * features, dim=1)\n        logits = self.classifier_fc(pooled)\n\n        return logits, alpha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T18:33:15.391729Z","iopub.execute_input":"2025-07-31T18:33:15.392164Z","iopub.status.idle":"2025-07-31T18:33:15.401825Z","shell.execute_reply.started":"2025-07-31T18:33:15.392137Z","shell.execute_reply":"2025-07-31T18:33:15.401177Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\ndef train(model, train_loader, val_loader, device, num_epochs=10, lr=1e-4):\n    \"\"\"Training and validation loop for the model.\"\"\"\n    \n    # MODIFICATION: Use CrossEntropyLoss for multi-class classification\n    criterion = nn.CrossEntropyLoss()\n    \n    # MODIFICATION: Optimizer only targets parameters that are not frozen\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=1e-5)\n    \n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-6\n    )\n    \n    scaler = GradScaler()\n    model.to(device)\n\n    for epoch in range(1, num_epochs + 1):\n        # --- Training phase ---\n        model.train()\n        train_loss, train_correct, total = 0.0, 0, 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\")\n\n        for X, y in pbar:\n            X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            optimizer.zero_grad()\n\n            with autocast():\n                outputs, _ = model(X)\n                # MODIFICATION: Use CrossEntropyLoss directly with logits and integer labels\n                loss = criterion(outputs, y)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            train_loss += loss.item() * X.size(0)\n            \n            # MODIFICATION: Get predictions for multi-class\n            preds = torch.argmax(outputs, 1)\n            train_correct += (preds == y).sum().item()\n            total += y.size(0)\n\n            pbar.set_postfix(loss=loss.item(), acc=100 * train_correct / total)\n\n        avg_train_loss = train_loss / total\n        train_acc = 100 * train_correct / total\n\n        # --- Validation phase ---\n        model.eval()\n        val_loss, val_correct, total_val = 0.0, 0, 0\n\n        with torch.no_grad(), autocast():\n            for X_val, y_val in val_loader:\n                X_val, y_val = X_val.to(device, non_blocking=True), y_val.to(device, non_blocking=True)\n                outputs, _ = model(X_val)\n                loss = criterion(outputs, y_val)\n\n                val_loss += loss.item() * X_val.size(0)\n                preds = torch.argmax(outputs, 1)\n                val_correct += (preds == y_val).sum().item()\n                total_val += y_val.size(0)\n\n        avg_val_loss = val_loss / total_val\n        val_acc = 100 * val_correct / total_val\n\n        print(f\"Epoch {epoch:02d}: \"\n              f\"Train Loss {avg_train_loss:.4f} Acc {train_acc:.2f}%, \"\n              f\"Val Loss {avg_val_loss:.4f} Acc {val_acc:.2f}%\")\n\n        scheduler.step(avg_val_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T18:33:15.402492Z","iopub.execute_input":"2025-07-31T18:33:15.402656Z","iopub.status.idle":"2025-07-31T18:33:15.430460Z","shell.execute_reply.started":"2025-07-31T18:33:15.402643Z","shell.execute_reply":"2025-07-31T18:33:15.429959Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nweights_path = \"/kaggle/input/modifed-axial/pytorch/default/2/axial_weights(1).pth\" \n\n# 1. Instantiate model for the new task (3 classes)\nmodel = AttentionXAIClassifier(num_classes=3)\n\n# 2. Load the state dictionary from the pre-trained model\n# Ensure the weights file 'axial_weights.pth' is in the root directory or provide the correct path.\nif os.path.exists(weights_path):\n    print(f\"Loading pre-trained weights from {weights_path}\")\n    state_dict = torch.load(weights_path, map_location=device)\n    \n    # Handle cases where weights were saved with DataParallel\n    if all(key.startswith('module.') for key in state_dict.keys()):\n        print(\"Removing 'module.' prefix from state_dict keys.\")\n        state_dict = {k.partition('module.')[2]: v for k, v in state_dict.items()}\n\n    # FIX: Remove the weights of the final classification layer from the loaded state_dict\n    # as its size is different in the new model (1 output vs 3 outputs).\n    state_dict.pop('classifier_fc.4.weight', None)\n    state_dict.pop('classifier_fc.4.bias', None)\n\n    # Now, load the weights. strict=False will gracefully ignore the missing keys.\n    model.load_state_dict(state_dict, strict=False)\n    print(\"Weights loaded successfully.\")\n\n    # 3. Freeze the feature extraction layers\n    print(\"Freezing feature extraction layers (cnn_per_slice).\")\n    for param in model.cnn_per_slice.parameters():\n        param.requires_grad = False\n    # Optionally, you can also freeze the attention layers if you want\n    # for param in model.attn_fc.parameters():\n    #     param.requires_grad = False\nelse:\n    print(f\"WARNING: Pre-trained weights not found at '{weights_path}'. Training from scratch.\")\n\n# Handle multi-GPU\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n    model = torch.nn.DataParallel(model)\n\nmodel.to(device) \n\n# 4. Train the model (only the unfrozen layers will be updated)\ntrain(model, train_loader, val_loader, device, num_epochs=10, lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T18:33:15.432544Z","iopub.execute_input":"2025-07-31T18:33:15.432733Z","iopub.status.idle":"2025-07-31T20:33:16.294417Z","shell.execute_reply.started":"2025-07-31T18:33:15.432719Z","shell.execute_reply":"2025-07-31T20:33:16.287375Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n100%|██████████| 230M/230M [00:01<00:00, 217MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Loading pre-trained weights from /kaggle/input/modifed-axial/pytorch/default/2/axial_weights(1).pth\nRemoving 'module.' prefix from state_dict keys.\nWeights loaded successfully.\nFreezing feature extraction layers (cnn_per_slice).\nUsing 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2419306444.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\nEpoch 1/10 [Train]:   0%|          | 0/405 [00:00<?, ?it/s]/tmp/ipykernel_36/2419306444.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 1/10 [Train]: 100%|██████████| 405/405 [10:41<00:00,  1.58s/it, acc=59.4, loss=0.959]\n/tmp/ipykernel_36/2419306444.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.no_grad(), autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01: Train Loss 0.7909 Acc 59.36%, Val Loss 0.7585 Acc 58.01%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10 [Train]: 100%|██████████| 405/405 [10:44<00:00,  1.59s/it, acc=62.4, loss=0.726]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02: Train Loss 0.7296 Acc 62.45%, Val Loss 0.7327 Acc 66.23%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10 [Train]: 100%|██████████| 405/405 [10:26<00:00,  1.55s/it, acc=65.7, loss=0.513]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03: Train Loss 0.7080 Acc 65.72%, Val Loss 0.7356 Acc 67.10%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10 [Train]: 100%|██████████| 405/405 [10:25<00:00,  1.54s/it, acc=66.4, loss=0.399]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04: Train Loss 0.7059 Acc 66.40%, Val Loss 0.7375 Acc 64.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10 [Train]: 100%|██████████| 405/405 [10:26<00:00,  1.55s/it, acc=64.9, loss=0.55] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 05: Train Loss 0.6986 Acc 64.92%, Val Loss 0.7364 Acc 65.37%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10 [Train]: 100%|██████████| 405/405 [10:27<00:00,  1.55s/it, acc=66.5, loss=0.409]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06: Train Loss 0.6862 Acc 66.52%, Val Loss 0.7358 Acc 66.23%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10 [Train]: 100%|██████████| 405/405 [10:25<00:00,  1.54s/it, acc=64.7, loss=0.822]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07: Train Loss 0.6941 Acc 64.73%, Val Loss 0.7323 Acc 66.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10 [Train]: 100%|██████████| 405/405 [10:21<00:00,  1.53s/it, acc=67, loss=0.565]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 08: Train Loss 0.6786 Acc 67.02%, Val Loss 0.7388 Acc 66.23%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10 [Train]: 100%|██████████| 405/405 [10:14<00:00,  1.52s/it, acc=66.3, loss=1.25] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 09: Train Loss 0.6830 Acc 66.34%, Val Loss 0.7280 Acc 65.37%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10 [Train]: 100%|██████████| 405/405 [10:15<00:00,  1.52s/it, acc=65.7, loss=0.866]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss 0.6918 Acc 65.66%, Val Loss 0.7297 Acc 67.10%\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"torch.save(model.state_dict(), \"axial_weights_3class_finetuned.pth\")\nprint(\"Saved fine-tuned 3-class model weights.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:33:16.301431Z","iopub.execute_input":"2025-07-31T20:33:16.301744Z","iopub.status.idle":"2025-07-31T20:33:16.758207Z","shell.execute_reply.started":"2025-07-31T20:33:16.301715Z","shell.execute_reply":"2025-07-31T20:33:16.757452Z"}},"outputs":[{"name":"stdout","text":"Saved fine-tuned 3-class model weights.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"model.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for X, y in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n        X, y = X.to(device), y.to(device)\n        outputs, _ = model(X)\n        \n        # MODIFICATION: Get predictions for multi-class\n        preds = torch.argmax(outputs, 1)\n\n        all_preds.append(preds.cpu())\n        all_labels.append(y.cpu())\n\n# Concatenate all batches\nall_preds = torch.cat(all_preds).numpy()\nall_labels = torch.cat(all_labels).numpy()\n\n# MODIFICATION: Update target names for the 3-class report\ntarget_names = [\"AD\", \"CN\", \"MCI\"]\nprint(\"\\n--- Classification Report ---\")\nprint(classification_report(all_labels, all_preds, target_names=target_names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:33:16.758932Z","iopub.execute_input":"2025-07-31T20:33:16.759162Z","iopub.status.idle":"2025-07-31T20:36:12.730973Z","shell.execute_reply.started":"2025-07-31T20:33:16.759143Z","shell.execute_reply":"2025-07-31T20:36:12.730199Z"}},"outputs":[{"name":"stderr","text":"Evaluating on Test Set: 100%|██████████| 111/111 [02:55<00:00,  1.58s/it]","output_type":"stream"},{"name":"stdout","text":"\n--- Classification Report ---\n              precision    recall  f1-score   support\n\n          AD       0.60      0.62      0.61        73\n          CN       0.67      0.83      0.74       166\n         MCI       0.66      0.52      0.58       205\n\n    accuracy                           0.65       444\n   macro avg       0.64      0.66      0.64       444\nweighted avg       0.65      0.65      0.65       444\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"print(\"\\n--- Confusion Matrix ---\")\ncm = confusion_matrix(all_labels, all_preds)\n\n# MODIFICATION: Update display labels for the 3-class matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\ndisp.plot(cmap=\"Blues\", values_format='d')\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:36:12.731942Z","iopub.execute_input":"2025-07-31T20:36:12.732604Z","iopub.status.idle":"2025-07-31T20:36:14.256315Z","shell.execute_reply.started":"2025-07-31T20:36:12.732578Z","shell.execute_reply":"2025-07-31T20:36:14.255616Z"}},"outputs":[{"name":"stdout","text":"\n--- Confusion Matrix ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAgwAAAHHCAYAAADTQQDlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABPLElEQVR4nO3deXhM59sH8O+ZkMk6iRBZyGpJYikVpUGRCkEoRS2NNvaq2Jeqt2KJJb+iqJ021kpVW7RotUQrVbEvRZPYYikSmsgu65z3D820I2HOmEkmyXw/rnNd5jnPnHOPhNzuZzmCKIoiiIiIiJ5DZugAiIiIqOJjwkBEREQaMWEgIiIijZgwEBERkUZMGIiIiEgjJgxERESkERMGIiIi0ogJAxEREWnEhIGIiIg0YsJAVAauXr2KLl26wMbGBoIgYM+ePXq9/s2bNyEIAjZv3qzX61ZmHTt2RMeOHQ0dBlGVxYSBqqzr16/jvffeg6enJ8zMzKBQKNC2bVt8+umnePz4cZneOyQkBBcvXsSCBQuwbds2tGzZskzvV56GDBkCQRCgUChK/XO8evUqBEGAIAhYsmSJ1te/d+8e5syZg/Pnz+shWiLSl2qGDoCoLOzfvx9vvfUW5HI53n33XTRp0gT5+fk4evQopk2bhsuXL2PDhg1lcu/Hjx8jNjYWH330EcaOHVsm93Bzc8Pjx49RvXr1Mrm+JtWqVUNOTg727t2L/v37q53bvn07zMzMkJub+0LXvnfvHubOnQt3d3c0b95c8vt+/vnnF7ofEUnDhIGqnMTERAwcOBBubm44fPgwnJycVOdCQ0Nx7do17N+/v8zu//DhQwCAra1tmd1DEASYmZmV2fU1kcvlaNu2Lb788ssSCUNUVBSCgoLw7bfflkssOTk5sLCwgKmpabncj8hYcUiCqpxFixYhKysLkZGRaslCsfr162PChAmq14WFhZg3bx7q1asHuVwOd3d3/N///R/y8vLU3ufu7o4ePXrg6NGjaNWqFczMzODp6YmtW7eq+syZMwdubm4AgGnTpkEQBLi7uwN4Usov/v1/zZkzB4IgqLUdPHgQ7dq1g62tLaysrODl5YX/+7//U51/1hyGw4cP47XXXoOlpSVsbW3Rq1cvxMXFlXq/a9euYciQIbC1tYWNjQ2GDh2KnJycZ//BPuXtt9/Gjz/+iLS0NFXbqVOncPXqVbz99tsl+qempmLq1Klo2rQprKysoFAo0K1bN1y4cEHV59dff8Urr7wCABg6dKhqaKP4c3bs2BFNmjTBmTNn0L59e1hYWKj+XJ6ewxASEgIzM7MSnz8wMBA1atTAvXv3JH9WImLCQFXQ3r174enpiTZt2kjqP2LECMyaNQstWrTAsmXL0KFDB0RERGDgwIEl+l67dg39+vVD586d8cknn6BGjRoYMmQILl++DADo06cPli1bBgAYNGgQtm3bhuXLl2sV/+XLl9GjRw/k5eUhPDwcn3zyCd544w38/vvvz33foUOHEBgYiAcPHmDOnDmYPHkyjh07hrZt2+LmzZsl+vfv3x+ZmZmIiIhA//79sXnzZsydO1dynH369IEgCNi1a5eqLSoqCt7e3mjRokWJ/jdu3MCePXvQo0cPLF26FNOmTcPFixfRoUMH1Q9vHx8fhIeHAwBGjRqFbdu2Ydu2bWjfvr3qOikpKejWrRuaN2+O5cuXw9/fv9T4Pv30U9jb2yMkJARFRUUAgPXr1+Pnn3/GypUr4ezsLPmzEhEAkagKSU9PFwGIvXr1ktT//PnzIgBxxIgRau1Tp04VAYiHDx9Wtbm5uYkAxJiYGFXbgwcPRLlcLk6ZMkXVlpiYKAIQFy9erHbNkJAQ0c3NrUQMs2fPFv/7V3HZsmUiAPHhw4fPjLv4Hps2bVK1NW/eXKxdu7aYkpKiartw4YIok8nEd999t8T9hg0bpnbNN998U6xZs+Yz7/nfz2FpaSmKoij269dP7NSpkyiKolhUVCQ6OjqKc+fOLfXPIDc3VywqKirxOeRyuRgeHq5qO3XqVInPVqxDhw4iAHHdunWlnuvQoYNa208//SQCEOfPny/euHFDtLKyEnv37q3xMxJRSawwUJWSkZEBALC2tpbU/4cffgAATJ48Wa19ypQpAFBirkOjRo3w2muvqV7b29vDy8sLN27ceOGYn1Y89+G7776DUqmU9J779+/j/PnzGDJkCOzs7FTtL730Ejp37qz6nP81evRotdevvfYaUlJSVH+GUrz99tv49ddfkZSUhMOHDyMpKanU4QjgybwHmezJPzlFRUVISUlRDbecPXtW8j3lcjmGDh0qqW+XLl3w3nvvITw8HH369IGZmRnWr18v+V5E9C8mDFSlKBQKAEBmZqak/rdu3YJMJkP9+vXV2h0dHWFra4tbt26ptbu6upa4Ro0aNfDo0aMXjLikAQMGoG3bthgxYgQcHBwwcOBA7Ny587nJQ3GcXl5eJc75+Pjg77//RnZ2tlr705+lRo0aAKDVZ+nevTusra3x1VdfYfv27XjllVdK/FkWUyqVWLZsGRo0aAC5XI5atWrB3t4ef/zxB9LT0yXfs06dOlpNcFyyZAns7Oxw/vx5rFixArVr15b8XiL6FxMGqlIUCgWcnZ1x6dIlrd739KTDZzExMSm1XRTFF75H8fh6MXNzc8TExODQoUN455138Mcff2DAgAHo3Llzib660OWzFJPL5ejTpw+2bNmC3bt3P7O6AAALFy7E5MmT0b59e3zxxRf46aefcPDgQTRu3FhyJQV48uejjXPnzuHBgwcAgIsXL2r1XiL6FxMGqnJ69OiB69evIzY2VmNfNzc3KJVKXL16Va09OTkZaWlpqhUP+lCjRg21FQXFnq5iAIBMJkOnTp2wdOlS/Pnnn1iwYAEOHz6MX375pdRrF8eZkJBQ4lx8fDxq1aoFS0tL3T7AM7z99ts4d+4cMjMzS50oWuybb76Bv78/IiMjMXDgQHTp0gUBAQEl/kykJm9SZGdnY+jQoWjUqBFGjRqFRYsW4dSpU3q7PpExYcJAVc4HH3wAS0tLjBgxAsnJySXOX79+HZ9++imAJyV1ACVWMixduhQAEBQUpLe46tWrh/T0dPzxxx+qtvv372P37t1q/VJTU0u8t3gDo6eXehZzcnJC8+bNsWXLFrUfwJcuXcLPP/+s+pxlwd/fH/PmzcOqVavg6Oj4zH4mJiYlqhdff/017t69q9ZWnNiUllxpa/r06bh9+za2bNmCpUuXwt3dHSEhIc/8cySiZ+PGTVTl1KtXD1FRURgwYAB8fHzUdno8duwYvv76awwZMgQA0KxZM4SEhGDDhg1IS0tDhw4dcPLkSWzZsgW9e/d+5pK9FzFw4EBMnz4db775JsaPH4+cnBysXbsWDRs2VJv0Fx4ejpiYGAQFBcHNzQ0PHjzAmjVrULduXbRr1+6Z11+8eDG6desGPz8/DB8+HI8fP8bKlSthY2ODOXPm6O1zPE0mk2HmzJka+/Xo0QPh4eEYOnQo2rRpg4sXL2L79u3w9PRU61evXj3Y2tpi3bp1sLa2hqWlJVq3bg0PDw+t4jp8+DDWrFmD2bNnq5Z5btq0CR07dkRYWBgWLVqk1fWIjJ6BV2kQlZkrV66II0eOFN3d3UVTU1PR2tpabNu2rbhy5UoxNzdX1a+goECcO3eu6OHhIVavXl10cXERZ8yYodZHFJ8sqwwKCipxn6eX8z1rWaUoiuLPP/8sNmnSRDQ1NRW9vLzEL774osSyyujoaLFXr16is7OzaGpqKjo7O4uDBg0Sr1y5UuIeTy89PHTokNi2bVvR3NxcVCgUYs+ePcU///xTrU/x/Z5etrlp0yYRgJiYmPjMP1NRVF9W+SzPWlY5ZcoU0cnJSTQ3Nxfbtm0rxsbGlroc8rvvvhMbNWokVqtWTe1zdujQQWzcuHGp9/zvdTIyMkQ3NzexRYsWYkFBgVq/SZMmiTKZTIyNjX3uZyAidYIoajHDiYiIiIwS5zAQERGRRkwYiIiISCMmDERERKQREwYiIiLSiAkDERERacSEgYiIiDQy+o2blEol7t27B2tra71uSUtEROVDFEVkZmbC2dlZ9URUfcvNzUV+fr5ermVqagozMzO9XKs8GX3CcO/ePbi4uBg6DCIi0tGdO3dQt25dvV83NzcX5tY1gcIcvVzP0dERiYmJlS5pMPqEwdraGgCw4aczMLe0MnA0VNa6+Dz7WQdU9Vy5L+0x51S5ZWdlouurPqp/z/UtPz8fKMyBvFEIYCL90eqlKspH0p9bkJ+fz4ShsikehjC3tIKFVdl8s1HFoVAoDB0ClSOrLA4zGpMyH1auZgZBx4RBFCrv1EGjTxiIiIgkEQDompRU4hyWCQMREZEUguzJoes1KqnKGzkRERGVG1YYiIiIpBAEPQxJVN4xCSYMREREUnBIgoiIiOj5WGEgIiKSgkMSREREpJkehiQqcWG/8kZORERE5YYVBiIiIik4JEFEREQacZUEERER0fOxwkBERCQFhySIiIhIIyMfkmDCQEREJIWRVxgqb6pDRERE5YYVBiIiIik4JEFEREQaCYIeEgYOSRAREVEVxgoDERGRFDLhyaHrNSopJgxERERSGPkchsobOREREZUbVhiIiIikMPJ9GJgwEBERScEhCSIiIqLnY4WBiIhICg5JEBERkUZGPiTBhIGIiEgKI68wVN5Uh4iIiMoNKwxERERScEiCiIiINOKQBBEREdHzscJAREQkiR6GJCrx/9OZMBAREUnBIQkiIiKi52OFgYiISApB0MMqicpbYWDCQEREJIWRL6usvJETERFRuWGFgYiISAojn/TIhIGIiEgKIx+SYMJAREQkhZFXGCpvqkNERFTFxcTEoGfPnnB2doYgCNizZ4/qXEFBAaZPn46mTZvC0tISzs7OePfdd3Hv3j21a6SmpiI4OBgKhQK2trYYPnw4srKytI6FCQMREZEUxUMSuh5ayM7ORrNmzbB69eoS53JycnD27FmEhYXh7Nmz2LVrFxISEvDGG2+o9QsODsbly5dx8OBB7Nu3DzExMRg1apTWH59DEkRERFIYYEiiW7du6NatW6nnbGxscPDgQbW2VatWoVWrVrh9+zZcXV0RFxeHAwcO4NSpU2jZsiUAYOXKlejevTuWLFkCZ2dnybGwwkBERFTOMjIy1I68vDy9XDc9PR2CIMDW1hYAEBsbC1tbW1WyAAABAQGQyWQ4ceKEVtdmwkBERCSBIAh6OQDAxcUFNjY2qiMiIkLn+HJzczF9+nQMGjQICoUCAJCUlITatWur9atWrRrs7OyQlJSk1fU5JEFERCTBf3/g63ARAMCdO3dUP9QBQC6X63TZgoIC9O/fH6IoYu3atTpd61mYMBAREZUzhUKhljDoojhZuHXrFg4fPqx2XUdHRzx48ECtf2FhIVJTU+Ho6KjVfTgkQUREJIWgp0OPipOFq1ev4tChQ6hZs6baeT8/P6SlpeHMmTOqtsOHD0OpVKJ169Za3YsVBiIiIgn0OSQhVVZWFq5du6Z6nZiYiPPnz8POzg5OTk7o168fzp49i3379qGoqEg1L8HOzg6mpqbw8fFB165dMXLkSKxbtw4FBQUYO3YsBg4cqNUKCYAJAxERUYV1+vRp+Pv7q15PnjwZABASEoI5c+bg+++/BwA0b95c7X2//PILOnbsCADYvn07xo4di06dOkEmk6Fv375YsWKF1rEwYSAiIpLAEBWGjh07QhTFZ55/3rlidnZ2iIqK0uq+pWHCQEREJIEhEoaKhAmDEfl+3zF89c0v6Nr5FbwT3AUAMD9iG+ISbqv1e73jyxg+pLshQiQ9+2znEaz8IhoPUjLQpEEdfDztLfg2djd0WKSDLd/8iiOxl3Drr4eQy6ujqbcbxrzbFW517VV9Uh5lYtXmH3Dy/DXkPM6Dax17DHnLH/5tmhgw8sqPCUMFFRsbi3bt2qFr167Yv3+/qv3mzZvw8PBQvbaysoKrqys6duyIiRMnokGDBoYIt8K7fuMeDv96Fq4utUuc8+/QHP3e7KB6bSqvXp6hURnZ9fMZzFy+G0s/HADfJu5Y9+Uv6DtuNU59Mwv2dtaGDo9e0LlLN9C3ux98GtRFUZES67b9hIlzNiJq1SSYm5kCAMKX70Rmdi4WffQubBWW+DnmPGYujsLGT8bCy1O7iW5ExSrsssrIyEiMGzcOMTExJZ68BQCHDh3C/fv3ceHCBSxcuBBxcXFo1qwZoqOjDRBtxZabm48167/DiKFBsLQwK3FeblodtrZWqsPCXLcNRKhiWBN1GO/2boPgN/zg7emEpTMGwsLMFF98H2vo0EgHy+cMQ1AnX3i6OqCBhxNmTuiHpIdpiL9+V9XnYvxtvBXkh8YNXVDH0Q5D+78OK0szJFy7+5wrk0YVcFlleaqQCUNWVha++uorvP/++wgKCsLmzZtL9KlZsyYcHR3h6emJXr164dChQ2jdujWGDx+OoqKi8g+6Atu87QCaN6uPJo09Sj3/+/HLeG/sUkz/aAN2fP0L8vIKyjlC0rf8gkKcj7+Djq28VG0ymQwdWnnh1MVEA0ZG+paVkwsAUFiZq9qaervi0NE/kJ6ZA6VSiYMxF5CfX4iXm5b+bwBJo8+toSujCpkw7Ny5E97e3vDy8sLgwYOxceNGjTNBZTIZJkyYgFu3bqltUGHsYo9fRuKtJAzo51/q+TZ+jTFm1Bv4aPpgvBHUBkePXcSaDd+Vc5SkbylpWSgqUpYYerC3U+BBSoaBoiJ9UyqVWP75Przk44Z6bv/u2jd/2tsoKlSi6+B5aN8vDB+v3Y3/zRgMF6daBoyWKrsKOYchMjISgwcPBgB07doV6enpOHLkiGpN6bN4e3sDeDLPoVWrVqX2ycvLU3sqWEZG1f3HMyUlA1ujDmLGtEEwNS39S/16xxaq37u61IatrRUWLtqO5AeP4FC7RnmFSkQvYMn673HjdjLWR4xWa98QdRCZ2Y+xInw4bBWWiDlxGTMXf4m1C99DfXfttgOmfz15urWukx71E4shVLiEISEhASdPnsTu3bsBPHmq1oABAxAZGakxYSiuQjzvCxoREYG5c+fqLd6KLPHmfWRkZOOj2ZGqNqVSRPyV2/g5+jS2fP4hZDL1IlO9ek8mRCUnpzJhqMRq2lrBxESGh6mZau0PUzNQu6Z+9q8nw1qy/jv8fioeayNGoXYtG1X7X/dT8M3+WGxfORGerg4AgAYeTjh/+Sa+/SEW08e8aaiQKz0B+hhSqLwZQ4VLGCIjI1FYWKi2ZaUoipDL5Vi1atVz3xsXFwcAaqsonjZjxgzVTlnAkwqDi4uLjlFXTI0bueN/80eqtW2I3Acnx5roGeRXIlkAgFu3kwEAtrZW5RIjlQ3T6tXQ3NsFR04lIKhjMwBPytcxp65gxFvtDRwd6UIURXyy4XscOf4n1iwYCWcHO7Xzuf/MQZI99YPNRCaTtMkP0bNUqIShsLAQW7duxSeffIIuXbqonevduze+/PJLdO3atdT3KpVKrFixAh4eHnj55ZefeQ+5XK7zY0QrC3NzOVzqqi+jlJtWh7WVOVzq1kbyg0c4FnsJzZvVh5WlOW7/9QBfRB2Et5crXF0cDBQ16cuYt1/HmLnb8LKPK1o0dsfaL39B9uM8BPd81dChkQ6WrP8OP8dcwMf/9w4szOVIefSkimRpYQYzeXW417VHXaea+HjNbowd2h021haIOfEnTl64hiUz3zVw9JUb92GoQPbt24dHjx5h+PDhsLGxUTvXt29fREZGqhKGlJQUJCUlIScnB5cuXcLy5ctx8uRJ7N+/HyYmJoYIv9KpZmKCS3/exIGfTyEvLx92NRV4paU3er/RztChkR706eKLv9OysHD9fjxIyUTThnXwzYpQDklUcrt+PAEACP3oM7X2meP7IaiTL6pVM8HSWUOwZusBTJu/FY9z81DXqSbCJvRDm5behgi56tDHssjKmy9AECtQjapnz55QKpVqGzUVO3nyJFq3bo0LFy6gWbNmqnYLCwu4ubnB398fkyZNQv369bW6Z0ZGBmxsbLDtaAIsrLiZTVXXvbGToUOgchR/L1NzJ6r0sjIz8FqTukhPT4dCof+EuPjnRI2Bn0MwtdDpWmJ+Dh7tGFFmsZalClVh2Lt37zPPtWrVSjX+VoFyHCIiMhZ6GJIQOSRBRERUteljDkNl3riJCQMREZEExp4wVMidHomIiKhiYYWBiIhICiNfJcGEgYiISAIOSRARERFpwAoDERGRBMZeYWDCQEREJIGxJwwckiAiIiKNWGEgIiKSwNgrDEwYiIiIpDDyZZUckiAiIiKNWGEgIiKSgEMSREREpBETBiIiItLI2BMGzmEgIiIijVhhICIiksLIV0kwYSAiIpKAQxJEREREGrDCQEREJIGxVxiYMBAREUkgQA8JQyWexMAhCSIiItKIFQYiIiIJOCRBREREmhn5skoOSRAREZFGrDAQERFJwCEJIiIi0ogJAxEREWkkCE8OXa9RWXEOAxEREWnECgMREZEETyoMug5J6CkYA2CFgYiISArh32GJFz20XVYZExODnj17wtnZGYIgYM+ePWrnRVHErFmz4OTkBHNzcwQEBODq1atqfVJTUxEcHAyFQgFbW1sMHz4cWVlZWn98JgxEREQVVHZ2Npo1a4bVq1eXen7RokVYsWIF1q1bhxMnTsDS0hKBgYHIzc1V9QkODsbly5dx8OBB7Nu3DzExMRg1apTWsXBIgoiISAJDrJLo1q0bunXrVuo5URSxfPlyzJw5E7169QIAbN26FQ4ODtizZw8GDhyIuLg4HDhwAKdOnULLli0BACtXrkT37t2xZMkSODs7S46FFQYiIiIJdB2O+O8qi4yMDLUjLy9P63gSExORlJSEgIAAVZuNjQ1at26N2NhYAEBsbCxsbW1VyQIABAQEQCaT4cSJE1rdjwkDERFROXNxcYGNjY3qiIiI0PoaSUlJAAAHBwe1dgcHB9W5pKQk1K5dW+18tWrVYGdnp+ojFYckiIiIJJDJBMhkug1JiP+8/86dO1AoFKp2uVyu03XLAysMREREEuhzSEKhUKgdL5IwODo6AgCSk5PV2pOTk1XnHB0d8eDBA7XzhYWFSE1NVfWRigkDERFRJeTh4QFHR0dER0er2jIyMnDixAn4+fkBAPz8/JCWloYzZ86o+hw+fBhKpRKtW7fW6n4ckiAiIpLAEKsksrKycO3aNdXrxMREnD9/HnZ2dnB1dcXEiRMxf/58NGjQAB4eHggLC4OzszN69+4NAPDx8UHXrl0xcuRIrFu3DgUFBRg7diwGDhyo1QoJgAkDERGRJIZ4lsTp06fh7++vej158mQAQEhICDZv3owPPvgA2dnZGDVqFNLS0tCuXTscOHAAZmZmqvds374dY8eORadOnSCTydC3b1+sWLFC69iZMBAREUlgiApDx44dIYric68XHh6O8PDwZ/axs7NDVFSUVvctDecwEBERkUasMBAREUlgiApDRcKEgYiISAJDzGGoSDgkQURERBqxwkBERCSBAD0MSWj7fOsKhAkDERGRBBySICIiItKAFQYiIiIJuEqCiIiINOKQBBEREZEGrDAQERFJwCEJIiIi0sjYhySYMBAREUlg7BUGzmEgIiIijVhh+EcXH0coFApDh0FlrMYrYw0dApWj2O8iDB0CVSV6GJKoxBs9MmEgIiKSgkMSRERERBqwwkBERCQBV0kQERGRRhySICIiItKAFQYiIiIJOCRBREREGnFIgoiIiEgDVhiIiIgkMPYKAxMGIiIiCTiHgYiIiDQy9goD5zAQERGRRqwwEBERScAhCSIiItKIQxJEREREGrDCQEREJIEAPQxJ6CUSw2DCQEREJIFMECDTMWPQ9f2GxCEJIiIi0ogVBiIiIgm4SoKIiIg0MvZVEkwYiIiIJJAJTw5dr1FZcQ4DERERacQKAxERkRSCHoYUKnGFgQkDERGRBMY+6ZFDEkRERKQRKwxEREQSCP/80vUalRUTBiIiIgm4SoKIiIgqpKKiIoSFhcHDwwPm5uaoV68e5s2bB1EUVX1EUcSsWbPg5OQEc3NzBAQE4OrVq3qPhQkDERGRBMUbN+l6aOPjjz/G2rVrsWrVKsTFxeHjjz/GokWLsHLlSlWfRYsWYcWKFVi3bh1OnDgBS0tLBAYGIjc3V6+fX9KQxPfffy/5gm+88cYLB0NERFRRGWKVxLFjx9CrVy8EBQUBANzd3fHll1/i5MmTAJ5UF5YvX46ZM2eiV69eAICtW7fCwcEBe/bswcCBA3UL+D8kJQy9e/eWdDFBEFBUVKRLPERERFVeRkaG2mu5XA65XF6iX5s2bbBhwwZcuXIFDRs2xIULF3D06FEsXboUAJCYmIikpCQEBASo3mNjY4PWrVsjNja2/BMGpVKptxsSERFVRvp8vLWLi4ta++zZszFnzpwS/T/88ENkZGTA29sbJiYmKCoqwoIFCxAcHAwASEpKAgA4ODiovc/BwUF1Tl90WiWRm5sLMzMzfcVCRERUYelzSOLOnTtQKBSq9tKqCwCwc+dObN++HVFRUWjcuDHOnz+PiRMnwtnZGSEhIboFoyWtJz0WFRVh3rx5qFOnDqysrHDjxg0AQFhYGCIjI/UeIBERUUWgz0mPCoVC7XhWwjBt2jR8+OGHGDhwIJo2bYp33nkHkyZNQkREBADA0dERAJCcnKz2vuTkZNU5fdE6YViwYAE2b96MRYsWwdTUVNXepEkTfP7553oNjoiIyJjl5ORAJlP/UW1iYqKaKuDh4QFHR0dER0erzmdkZODEiRPw8/PTayxaD0ls3boVGzZsQKdOnTB69GhVe7NmzRAfH6/X4IiIiCoKQ6yS6NmzJxYsWABXV1c0btwY586dw9KlSzFs2LB/ridg4sSJmD9/Pho0aAAPDw+EhYXB2dlZ8oIFqbROGO7evYv69euXaFcqlSgoKNBLUERERBWNPic9SrVy5UqEhYVhzJgxePDgAZydnfHee+9h1qxZqj4ffPABsrOzMWrUKKSlpaFdu3Y4cOCA3ucYap0wNGrUCL/99hvc3NzU2r/55hu8/PLLeguMiIjI2FlbW2P58uVYvnz5M/sIgoDw8HCEh4eXaSxaJwyzZs1CSEgI7t69C6VSiV27diEhIQFbt27Fvn37yiJGIiIigxP+OXS9RmWl9aTHXr16Ye/evTh06BAsLS0xa9YsxMXFYe/evejcuXNZxEhERGRwhtgauiJ5oX0YXnvtNRw8eFDfsRAREVEF9cIbN50+fRpxcXEAnsxr8PX11VtQREREFY2xP95a64Thr7/+wqBBg/D777/D1tYWAJCWloY2bdpgx44dqFu3rr5jJCIiMjh9DClU5iEJrecwjBgxAgUFBYiLi0NqaipSU1MRFxcHpVKJESNGlEWMREREZGBaVxiOHDmCY8eOwcvLS9Xm5eWFlStX4rXXXtNrcERERBVJJS4Q6EzrhMHFxaXUDZqKiorg7Oysl6CIiIgqGg5JaGnx4sUYN24cTp8+rWo7ffo0JkyYgCVLlug1OCIiooqieNKjrkdlJanCUKNGDbWsKDs7G61bt0a1ak/eXlhYiGrVqmHYsGF637uaiIiIDE9SwvC8LSmJiIiMgbEPSUhKGEJCQso6DiIiogrN2LeGfuGNmwAgNzcX+fn5am0KhUKngIiIiKji0TphyM7OxvTp07Fz506kpKSUOF9UVKSXwIiIiCoSQzzeuiLRepXEBx98gMOHD2Pt2rWQy+X4/PPPMXfuXDg7O2Pr1q1lESMREZHBCYJ+jspK6wrD3r17sXXrVnTs2BFDhw7Fa6+9hvr168PNzQ3bt29HcHBwWcRJREREBqR1hSE1NRWenp4AnsxXSE1NBQC0a9cOMTEx+o2OiIioguDjrbXk6emJxMREuLq6wtvbGzt37kSrVq2wd+9e1cOoqGL7bOcRrPwiGg9SMtCkQR18PO0t+DZ2N3RYpIU2L9fDuHcC0MzbFU72NgieugE/HPlDdX76yO7o06UF6jjUQEFBEc7H38b8NXtx5vItVZ96rrURPr43WjfzRPVqJvjz2j0sWLcPR89cNcRHIom2fPMrjsRewq2/HkIur46m3m4Y825XuNW1V/VJeZSJVZt/wMnz15DzOA+udewx5C1/+LdpYsDIKz99DClU4nxB+wrD0KFDceHCBQDAhx9+iNWrV8PMzAyTJk3CtGnT9B4g6deun89g5vLdmD6iG37dNh1NGtRB33Gr8TA109ChkRYszOW4dOUupi36qtTz128/wAeLv0bbQQvRbeRS3L6Xil2rxqKmrZWqz46lo1HNRIZe76+A/7uLcOnqXexYNhq1a1qX18egF3Du0g307e6HzxaPwadzh6OwsAgT52zE49x/V6yFL9+JW3f/xqKP3sUXKyaio19jzFwchYQb9wwYOVV2WicMkyZNwvjx4wEAAQEBiI+PR1RUFM6dO4cJEyboLbCkpCSMGzcOnp6ekMvlcHFxQc+ePREdHQ0AcHd3hyAIOH78uNr7Jk6ciI4dO+otjqpmTdRhvNu7DYLf8IO3pxOWzhgICzNTfPF9rKFDIy0cOvYnFqzbh/2//lHq+W9+Oo0jJxNw624K4m8kYebyXVBYmaNxgyfPe7GzsUR9t9pYvuUgLl+7hxt3HmLuqu9gaS6HTz0+E6YiWz5nGII6+cLT1QENPJwwc0I/JD1MQ/z1u6o+F+Nv460gPzRu6II6jnYY2v91WFmaIeHa3edcmTQpXiWh61FZaZ0wPM3NzQ19+vTBSy+9pI94AAA3b96Er68vDh8+jMWLF+PixYs4cOAA/P39ERoaqupnZmaG6dOn6+2+VV1+QSHOx99Bx1b/PmlUJpOhQysvnLqYaMDIqCxVr2aCkDfbIj0zB5euPPmBkZqejSs3kzAgqBUszExhYiLDkD7t8CAlA+fjbhs4YtJGVk4uAEBhZa5qa+rtikNH/0B6Zg6USiUOxlxAfn4hXm7qYagwqwSukpBgxYoVki9YXH3QxZgxYyAIAk6ePAlLS0tVe+PGjTFs2DDV61GjRmHdunX44Ycf0L17d53vW9WlpGWhqEgJezv1krO9nQJXbyYbKCoqK4HtmuDzBUNhYVYdSX9n4M2xq5Canq06/2boKnyxeBTuHFkCpVLEw0dZ6Dd+DdIzHxswatKGUqnE8s/34SUfN9Rzc1S1z5/2NsIWf4mug+fBxEQGM3l1/G/GYLg41TJgtJUft4aWYNmyZZIuJgiCzglDamoqDhw4gAULFqglC8X+O7HSw8MDo0ePxowZM9C1a1fIZJoLJnl5ecjLy1O9zsjI0Cleoorqt9NX0D44AjVtrfBu7zbYtHAYAoYuwd+PsgAAiz/oj78fZaL7yOV4nJePd3u3wZdL30OnkMVITuHfi8pgyfrvceN2MtZHjFZr3xB1EJnZj7EifDhsFZaIOXEZMxd/ibUL30N9d8dnXI3o+SQNSSQmJko6bty4oXNA165dgyiK8Pb2ltR/5syZSExMxPbt2yX1j4iIgI2NjepwcXHRJdxKpaatFUxMZCUmOD5MzUDtmtzSu6rJyc1H4l9/4/Slmxg/PwqFRUq806sNAKD9Kw0R2K4Jhn+0CSf+uIE/Ev7C1I93IjevAIN6tDZw5CTFkvXf4fdT8Vg9fyRq17JRtf91PwXf7I/FR+P74ZVm9dHAwwnDBwbAu14dfPsD5yrpQqano7KqcLGLoqhVf3t7e0ydOhWzZs0q8VyL0syYMQPp6emq486dOy8aaqVjWr0amnu74MipBFWbUqlEzKkreIVjm1WeTCbAtPqToqKFmSmAJ1///1KKYqWelGUMRFHEkvXf4cjxP7Fq/gg4O9ipnc/NKwBQcgtiE5lM639fSZ2x78NQ4RKGBg0aQBAExMfHS37P5MmT8fjxY6xZs0ZjX7lcDoVCoXYYkzFvv46te47hy33HkZCYhMn/+wrZj/MQ3PNVQ4dGWrA0N0WThnXQpGEdAICbc000aVgHdR1qwMLMFGFjeqJlE3e4ONZAM28XrAwLhpO9Lb6LPgsAOPlHItIyc7Bmzrto0qCOak8GN+ea+Pn3y4b8aKTBkvXf4acj5zF3ygBYmMuR8igTKY8yVYmCe1171HWqiY/X7MblK3fw1/0URO35DScvXEP71o0MHD1VZjo9rbIs2NnZITAwEKtXr8b48eNLzGNIS0srsUGUlZUVwsLCMGfOHLzxxhvlGG3l06eLL/5Oy8LC9fvxICUTTRvWwTcrQjkkUck093HDvvX/LmNeOLkvACBq33FMjtiBBu4OGBjUGjVtLZGanoNzf95C91HLEH8jCcCTVRL9xq/BzPd74rs141GtmgzxN5IQPHUDLl3l0ruKbNePJwAAoR99ptY+c3w/BHXyRbVqJlg6awjWbD2AafO34nFuHuo61UTYhH5o01LaUC+VThAAmRFv3CSIFbBGdePGDbRt2xZ2dnYIDw/HSy+9hMLCQhw8eBBr165FXFwc3N3dMXHiREycOBEAUFBQAB8fH9y9exetW7fGr7/+KuleGRkZsLGxQXJKutFVG4xRjVfGGjoEKkex30UYOgQqB1mZGXitSV2kp5fNv+PFPyfGfHkKcgsrzW94jrycLKwZ9EqZxVqWKtyQBPBk++mzZ8/C398fU6ZMQZMmTdC5c2dER0dj7dq1pb6nevXqmDdvHnJzc8s5WiIioqrvhYYkfvvtN6xfvx7Xr1/HN998gzp16mDbtm3w8PBAu3bt9BKYk5MTVq1ahVWrVpV6/ubNmyXaBg0ahEGDBunl/kRERP9l7PswaF1h+PbbbxEYGAhzc3OcO3dOtadBeno6Fi5cqPcAiYiIKgKZoJ+jstI6YZg/fz7WrVuHzz77DNWrV1e1t23bFmfPntVrcERERFQxaD0kkZCQgPbt25dot7GxQVpamj5iIiIiqnD4eGstOTo64tq1ayXajx49Ck9PT70ERUREVNHwaZVaGjlyJCZMmIATJ05AEATcu3cP27dvx9SpU/H++++XRYxEREQGZ+xbQ2s9JPHhhx9CqVSiU6dOyMnJQfv27SGXyzF16lSMGzeuLGIkIiIiA9M6YRAEAR999BGmTZuGa9euISsrC40aNYKVlW6bWRAREVVkxj6H4YW3hjY1NUWjRtyXnIiIjIMMus9BkKHyZgxaJwz+/v7P3Xji8OHDOgVEREREFY/WCUPz5s3VXhcUFOD8+fO4dOkSQkJC9BUXERFRhcIhCS0tW7as1PY5c+YgKytL54CIiIgqIn3s1GhUOz0+y+DBg7Fx40Z9XY6IiIgqkBee9Pi02NhYmJmZ6etyREREFYogQOdJj0Y1JNGnTx+116Io4v79+zh9+jTCwsL0FhgREVFFYqg5DHfv3sX06dPx448/IicnB/Xr18emTZvQsmVLAE9+Ds+ePRufffYZ0tLS0LZtW6xduxYNGjTQLdinaJ0w2NjYqL2WyWTw8vJCeHg4unTporfAiIiIjN2jR4/Qtm1b+Pv748cff4S9vT2uXr2KGjVqqPosWrQIK1aswJYtW+Dh4YGwsDAEBgbizz//1GvlX6uEoaioCEOHDkXTpk3VgiUiIqrqDDHp8eOPP4aLiws2bdqkavPw8FD9XhRFLF++HDNnzkSvXr0AAFu3boWDgwP27NmDgQMH6hbwf2g16dHExARdunThUymJiMjoCHr6BQAZGRlqR15eXqn3/P7779GyZUu89dZbqF27Nl5++WV89tlnqvOJiYlISkpCQECAqs3GxgatW7dGbGysXj+/1qskmjRpghs3bug1CCIiooquuMKg6wEALi4usLGxUR0RERGl3vPGjRuq+Qg//fQT3n//fYwfPx5btmwBACQlJQEAHBwc1N7n4OCgOqcvWs9hmD9/PqZOnYp58+bB19cXlpaWaucVCoXegiMiIqqK7ty5o/bzUi6Xl9pPqVSiZcuWWLhwIQDg5ZdfxqVLl7Bu3bpy3yxRcoUhPDwc2dnZ6N69Oy5cuIA33ngDdevWRY0aNVCjRg3Y2tpyXgMREVVZ+qwwKBQKteNZCYOTk1OJ5zb5+Pjg9u3bAABHR0cAQHJyslqf5ORk1Tl9kVxhmDt3LkaPHo1ffvlFrwEQERFVBoIgPPdZSlKvoY22bdsiISFBre3KlStwc3MD8GQCpKOjI6Kjo1WPbsjIyMCJEyfw/vvv6xTr0yQnDKIoAgA6dOig1wCIiIiodJMmTUKbNm2wcOFC9O/fHydPnsSGDRuwYcMGAE8SkIkTJ2L+/Plo0KCBalmls7MzevfurddYtJrDoGtmRUREVFkZYlnlK6+8gt27d2PGjBkIDw+Hh4cHli9fjuDgYFWfDz74ANnZ2Rg1ahTS0tLQrl07HDhwQO+7L2uVMDRs2FBj0pCamqpTQERERBWRoXZ67NGjB3r06PGcawoIDw9HeHi4DpFpplXCMHfu3BI7PRIREVHVp1XCMHDgQNSuXbusYiEiIqqwZIKg88OndH2/IUlOGDh/gYiIjJkh5jBUJJL3YSheJUFERETGR3KFQalUlmUcREREFZseJj2iElcYtN4amoiIyBjJIECm4098Xd9vSEwYiIiIJDDUssqKQuunVRIREZHxYYWBiIhIAmNfJcGEgYiISAJj34eBQxJERESkESsMREREEhj7pEcmDERERBLIoIchiUq8rJJDEkRERKQRKwxEREQScEiCiIiINJJB97J8ZS7rV+bYiYiIqJywwkBERCSBIAgQdBxT0PX9hsSEgYiISAIBuj9ssvKmC0wYiIiIJOFOj0REREQasMJAREQkUeWtD+iOCQMREZEExr4PA4ckiIiISCNWGIiIiCTgskoiIiLSiDs9EhEREWnACgMREZEEHJIgIiIijYx9p0cOSRAREZFGrDD8I+5uBqwyDB0FlbWRs0MNHQKVox4fRxs6BCoHyvyccrkPhySIiIhII2NfJcGEgYiISAJjrzBU5mSHiIiIygkrDERERBIY+yoJJgxEREQS8OFTRERERBqwwkBERCSBDAJkOg4q6Pp+Q2LCQEREJAGHJIiIiIg0YIWBiIhIAuGfX7peo7JiwkBERCQBhySIiIiowvvf//4HQRAwceJEVVtubi5CQ0NRs2ZNWFlZoW/fvkhOTi6T+zNhICIikkD4Z5WELseLDkmcOnUK69evx0svvaTWPmnSJOzduxdff/01jhw5gnv37qFPnz76+LglMGEgIiKSoHhIQtdDW1lZWQgODsZnn32GGjVqqNrT09MRGRmJpUuX4vXXX4evry82bdqEY8eO4fjx43r85E8wYSAiIpLAUAlDaGgogoKCEBAQoNZ+5swZFBQUqLV7e3vD1dUVsbGxun7cEjjpkYiIqJxlZGSovZbL5ZDL5SX67dixA2fPnsWpU6dKnEtKSoKpqSlsbW3V2h0cHJCUlKTXeAFWGIiIiCQR9PQLAFxcXGBjY6M6IiIiStzvzp07mDBhArZv3w4zM7Py/rglsMJAREQkgUx4cuh6DeBJMqBQKFTtpVUXzpw5gwcPHqBFixaqtqKiIsTExGDVqlX46aefkJ+fj7S0NLUqQ3JyMhwdHXULtBRMGIiIiMqZQqFQSxhK06lTJ1y8eFGtbejQofD29sb06dPh4uKC6tWrIzo6Gn379gUAJCQk4Pbt2/Dz89N7zEwYiIiIJCjvnR6tra3RpEkTtTZLS0vUrFlT1T58+HBMnjwZdnZ2UCgUGDduHPz8/PDqq6/qFGdpmDAQERFJUBF3ely2bBlkMhn69u2LvLw8BAYGYs2aNfq9yT+YMBAREVUSv/76q9prMzMzrF69GqtXry7zezNhICIikkCA7g+PqsSPkmDCQEREJIU+V0lURtyHgYiIiDRihYGIiEiC8l4lUdEwYSAiIpKgIq6SKE9MGIiIiCQQoPukxUqcL3AOAxEREWnGCgMREZEEMgiQ6TimIKvENQYmDERERBJwSIKIiIhIA1YYiIiIpDDyEgMTBiIiIgmMfR8GDkkQERGRRqwwEBERSaGHjZsqcYGBCQMREZEURj6FgUMSREREpBkrDERERFIYeYmBCQMREZEExr5KggkDERGRBMb+tErOYSAiIiKNWGEgIiKSwMinMDBhICIiksTIMwYOSRAREZFGrDAQERFJwFUSREREpBFXSRARERFpwAoDERGRBEY+55EJAxERkSRGnjFwSIKIiIg0YoWBiIhIAq6SICIiIo2MfZUEEwYiIiIJjHwKA+cwEBERkWasMFRhuw8cx54DJ3H/wSMAgIdLbQzp/zr8fL0AAHn5BVi16QdEH/0DBYVFaNW8Aaa89wbsbK0NGTa9oOyMLJw8FIs7126hsKAQCjsbdOjVCfbOtQEABfn5OHnoOG7F30Du41xY2yrQuPVLaNSyiYEjp+d5pV5NjHy9ARq72MDBxhyjPz+BQxfvq/WZ0M0bA/zcoTCvjjOJKZj19QXcepgNAGhdvxa2j2tX6rXf/ORXXLydVtYfoeow8hIDE4YqzL6mDUa/E4i6TjUhisCPv5zFjP99gY2fjIWnqwNWbtyPY2cSMG/a27C0NMOyDd/jo4+3Y23EaEOHTlrKe5yL7zfugrNHHXQN7gkzC3NkpKZBbiZX9Tn+0++4l/gXOvbpDGtba/x1/Q5+338EltaWcPPyMGD09DzmpiaIu5uOr0/cwtrhrUucH9WpAULa18MH28/gTmoOJnX3wabRbdA1Ihr5hUqcTUzBqzN/VHvPpO4+8Gtoz2RBS8Y+6dGgQxJDhgyBIAgYPbrkD6jQ0FAIgoAhQ4ao2pKSkjBu3Dh4enpCLpfDxcUFPXv2RHR0tKqPu7s7li9fXg7RV3ztXvGBn68XXJxrwbVOLbw3uAvMzUzx55U7yMrOxb7oMxg3tDt8X6oH73p18H/j+uJi/G1cSrht6NBJSxd+PwdLGyt06NUJtes4QFFDgbr1XKGws1H1Sb6ThAbNvOHsXgfWtgr4+DZGTcdaeHA32YCRkyYxcQ+w7Ic4HPzjfqnnh3Soh9U/J+DQpSQk3MvA1C/OwMHGDJ2bOgEACopE/J2ZpzrSsvMR0NQJ357g33PSjsHnMLi4uGDHjh14/Pixqi03NxdRUVFwdXVVtd28eRO+vr44fPgwFi9ejIsXL+LAgQPw9/dHaGioIUKvVIqKlDj02wXk5uajsZcLEq7fRWFhEVo2q6/q41a3NhzsbXGZCUOlcyshEfZOtXHo6wPYtngjdq3/CvFnLqv1cXBxxK0rN5GdkQVRFHEv8S+kp6Shbj3XZ1yVKjqXmhaobWOGY1ceqtqycgtx4dYjvOxhV+p7OjV1gq2lKb49cau8wqwyildJ6HpUVgYfkmjRogWuX7+OXbt2ITg4GACwa9cuuLq6wsPj3zLpmDFjIAgCTp48CUtLS1V748aNMWzYsHKPu7K4fisJoz9ch/z8QpibmWLhh4Ph4eKAq4n3Ub2aCawtzdX629lYISUty0DR0ovKfJSBuNOX0NSvGZq388XDew9w7MBvkJmYoGFzbwBAm27t8du+XxC1bAsEmQyCALzW0x9Obs4Gjp5eVC1rMwDA35m5au1/Z+bB3lpe2lvw1quu+C0+GUnpuaWep2cz8ikMhq8wAMCwYcOwadMm1euNGzdi6NChqtepqak4cOAAQkND1ZKFYra2tpLvlZeXh4yMDLWjKnN1roVNS8dh/aL30btrayxY8TUS77AEXdWIooiaTvZ4pZMfajnZw8e3MbxbNELcmUuqPpdP/oEHfyWjy8DueHPUW3i1S1sc+yEGd2/cMWDkVJ4cbczwmrcDvj7OKiJpr0IkDIMHD8bRo0dx69Yt3Lp1C7///jsGDx6sOn/t2jWIoghvb2+d7xUREQEbGxvV4eLiovM1K7Lq1auhrlNNeNerg9HvBKKeuxO+3ncMNW2tUVBYhMzsx2r9U9OzUNPWykDR0ouysLZADfsaam22teyQlf6kWlRYUIhT0cfxape2cPPyQE2HWmjc6iV4Nq6PP46dN0DEpA/FlYXiSkOxWtZyPMzMK9G/b2s3pGXnI/pi6fMhSANBT0clVSESBnt7ewQFBWHz5s3YtGkTgoKCUKtWLdV5URT1dq8ZM2YgPT1dddy5Y1z/uxKVIgoKiuBVrw6qVTPBmT+uq87dvvsQyQ/T0NiLY9qVjYOLE9JS0tTa0lPSYGXzZImsUqmEUqmE8NQAqiAIev37ReXrTkoOHqTnok1De1WblbwamrnVwLnE1BL9+7Z2xe5Tt1Go5Nf8RQh6+lVZGXwOQ7Fhw4Zh7NixAIDVq1ernWvQoAEEQUB8fLzO95HL5ZDLSx/bq2rWbfsJr7ZoCAd7W+Q8zsPBmAs4dzkRS2cNgZWlGXp08sXKTT9AYWUOCwszLP9sL5p4uaIJE4ZKp+mrzfDdxl0499tpeDauj4d3HyD+7GW81qMjAMBUbgonN2ecOHgMJtWrwcrGGkm37uLqHwl4tUvpa/SpYrAwNYGb/b9VP5eaFvCpY4O0nHzcf/QYm49cx5guDXHzYRbupDxZVpmcnouDT1UR/BrWgmstS+yM5WRHejEVJmHo2rUr8vPzIQgCAgMD1c7Z2dkhMDAQq1evxvjx40vMY0hLS9NqHoOxeJSehfmffo2UR5mwtDBDPXdHLJ01BK80bwAAGDcsCIIg4KNFUSgoKPxn46ZeBo6aXoR9HQd0HtANp6Jjce7IaVjXUMAvsB3qv+Sl6vN6vy44FX0cv+w6iLzHubCysUbL11+FT8vGBoycNGnqWkNt46WP3mwKAPj2xG1MjzqLDdFXYW5qgvkDmkNhXh2nb6Rg2LpjyC9Uql3nrVfdcOZGCm484KTmF8VnSVQQJiYmiIuLU/3+aatXr0bbtm3RqlUrhIeH46WXXkJhYSEOHjyItWvXqt5L/5oxtu9zz8tNq2PKe72YJFQRbg3d4dbQ/ZnnLaws0aFXp/ILiPTixLW/UX/Cnuf2+fTHeHz64/MrsJO3ntFjVMbJEKskIiIisGvXLsTHx8Pc3Bxt2rTBxx9/DC+vf/8zkJubiylTpmDHjh3Iy8tDYGAg1qxZAwcHBx2jVVch5jAUUygUUCgUpZ7z9PTE2bNn4e/vjylTpqBJkybo3LkzoqOjsXbt2nKOlIiIjI4BJj0eOXIEoaGhOH78OA4ePIiCggJ06dIF2dnZqj6TJk3C3r178fXXX+PIkSO4d+8e+vTpo9tnLYUgGvmMp4yMDNjY2ODXP+7Ayrr0ZIWqji3n7xo6BCpHuw4mGDoEKgfK/Bzc/zwY6enpz/xPpy6Kf06cuXpf558TWZkZ8G3g9MKxPnz4ELVr18aRI0fQvn17pKenw97eHlFRUejXrx8AID4+Hj4+PoiNjcWrr76qU7z/VaEqDERERBWVPldJPL0fUF5eyWWwpUlPTwfwZG4fAJw5cwYFBQUICAhQ9fH29oarqytiY2P1+vmZMBAREUmhj22h/xmScHFxUdsTKCIiQuPtlUolJk6ciLZt26JJkydPmU1KSoKpqWmJif8ODg5ISkrS68evMJMeiYiIjMWdO3fUhiSkLPcPDQ3FpUuXcPTo0bIM7ZmYMBAREUmgz1USz5vkX5qxY8di3759iImJQd26dVXtjo6OyM/PL7G9QHJyMhwdHXWMVh2HJIiIiKQwwCoJURQxduxY7N69G4cPH1Z7KCMA+Pr6onr16oiOjla1JSQk4Pbt2/Dz83uBD/lsrDAQERFVUKGhoYiKisJ3330Ha2tr1bwEGxsbmJubw8bGBsOHD8fkyZNhZ2cHhUKBcePGwc/PT68rJAAmDERERJLo41kQ2r6/eJ+hjh07qrVv2rQJQ4YMAQAsW7YMMpkMffv2Vdu4Sd+YMBAREUlgiK2hpWyVZGZmhtWrV5d4DpO+cQ4DERERacQKAxERkQSGeJZERcKEgYiISAojzxiYMBAREUlgiEmPFQnnMBAREZFGrDAQERFJIEAPqyT0EolhMGEgIiKSwMinMHBIgoiIiDRjhYGIiEgCQ2zcVJEwYSAiIpLEuAclOCRBREREGrHCQEREJAGHJIiIiEgj4x6Q4JAEERERScAKAxERkQQckiAiIiKNjP1ZEkwYiIiIpDDySQycw0BEREQascJAREQkgZEXGJgwEBERSWHskx45JEFEREQascJAREQkAVdJEBERkWZGPomBQxJERESkESsMREREEhh5gYEJAxERkRRcJUFERESkASsMREREkui+SqIyD0owYSAiIpKAQxJEREREGjBhICIiIo04JEFERCSBsQ9JMGEgIiKSwNi3huaQBBEREWnECgMREZEEHJIgIiIijYx9a2gOSRAREZFGrDAQERFJYeQlBiYMREREEnCVBBEREZEGrDAQERFJwFUSREREpJGRT2HgkAQREZEkgp6OF7B69Wq4u7vDzMwMrVu3xsmTJ3X6KC+CCQMREVEF9tVXX2Hy5MmYPXs2zp49i2bNmiEwMBAPHjwo1ziYMBAREUkg6OmXtpYuXYqRI0di6NChaNSoEdatWwcLCwts3LixDD7lszFhICIikqB40qOuhzby8/Nx5swZBAQEqNpkMhkCAgIQGxur50/4fEY/6VEURQBAdlamgSOh8pCfk2XoEKgcKfNzDB0ClYPir3Pxv+dlJSMjQ2/XePpacrkccrm8RP+///4bRUVFcHBwUGt3cHBAfHy8zvFow+gThszMJ4lCUJtGBo6EiIh0kZmZCRsbG71f19TUFI6Ojmjg4aKX61lZWcHFRf1as2fPxpw5c/Ry/bJi9AmDs7Mz7ty5A2trawiVeYGsljIyMuDi4oI7d+5AoVAYOhwqQ/xaGw9j/VqLoojMzEw4OzuXyfXNzMyQmJiI/Px8vVxPFMUSP29Kqy4AQK1atWBiYoLk5GS19uTkZDg6OuolHqmMPmGQyWSoW7euocMwGIVCYVT/sBgzfq2NhzF+rcuisvBfZmZmMDMzK9N7lMbU1BS+vr6Ijo5G7969AQBKpRLR0dEYO3ZsucZi9AkDERFRRTZ58mSEhISgZcuWaNWqFZYvX47s7GwMHTq0XONgwkBERFSBDRgwAA8fPsSsWbOQlJSE5s2b48CBAyUmQpY1JgxGSi6XY/bs2c8cN6Oqg19r48GvddU1duzYch+CeJoglvU6FCIiIqr0uHETERERacSEgYiIiDRiwkBEREQaMWEgIiIijZgwVHGxsbEwMTFBUFCQWvvNmzchCILqsLa2RuPGjREaGoqrV68aKFrSRVJSEsaNGwdPT0/I5XK4uLigZ8+eiI6OBgC4u7tDEAQcP35c7X0TJ05Ex44dDRAxSTVkyBAIgoDRo0eXOBcaGgpBEDBkyBBVm6bvBeDJ98Py5cvLIXqqKpgwVHGRkZEYN24cYmJicO/evRLnDx06hPv37+PChQtYuHAh4uLi0KxZM7V/WKjiu3nzJnx9fXH48GEsXrwYFy9exIEDB+Dv74/Q0FBVPzMzM0yfPt2AkdKLcnFxwY4dO/D48WNVW25uLqKiouDq6qpqk/q9QKQt7sNQhWVlZeGrr77C6dOnkZSUhM2bN+P//u//1PrUrFlTtR+5p6cnevbsiU6dOmH48OG4fv06TExMDBE6aWnMmDEQBAEnT56EpaWlqr1x48YYNmyY6vWoUaOwbt06/PDDD+jevbshQqUX1KJFC1y/fh27du1CcHAwAGDXrl1wdXWFh4eHqp/U7wUibbHCUIXt3LkT3t7e8PLywuDBg7Fx40aNj3+VyWSYMGECbt26hTNnzpRTpKSL1NRUHDhwAKGhoWo/IIrZ2tqqfu/h4YHRo0djxowZUCqV5Rgl6cOwYcOwadMm1euNGzeqbQ+szfcCkbaYMFRhkZGRGDx4MACga9euSE9Px5EjRzS+z9vbG8CT0iZVfNeuXYMoiqqvmyYzZ85EYmIitm/fXsaRkb4NHjwYR48exa1bt3Dr1i38/vvvqr/jgPbfC0TaYMJQRSUkJODkyZMYNGgQAKBatWoYMGAAIiMjNb63uAphTI/7rsy03azV3t4eU6dOxaxZs/T2uF4qH/b29ggKCsLmzZuxadMmBAUFoVatWqrz3LiXyhLnMFRRkZGRKCwsVHs+vCiKkMvlWLVq1XPfGxcXBwBq46JUcTVo0ACCICA+Pl7yeyZPnow1a9ZgzZo1ZRgZlYVhw4apnimwevVqtXMv8r1AJBUrDFVQYWEhtm7dik8++QTnz59XHRcuXICzszO+/PLLZ75XqVRixYoV8PDwwMsvv1yOUdOLsrOzQ2BgIFavXo3s7OwS59PS0kq0WVlZISwsDAsWLEBmZmY5REn60rVrV+Tn56OgoACBgYFq517ke4FIKiYMVdC+ffvw6NEjDB8+HE2aNFE7+vbtqzYskZKSgqSkJNy4cQPff/89AgICcPLkSURGRnKFRCWyevVqFBUVoVWrVvj2229x9epVxMXFYcWKFfDz8yv1PaNGjYKNjQ2ioqLKOVrShYmJCeLi4vDnn3+W+nf0Rb4XiKTgkEQVFBkZiYCAANjY2JQ417dvXyxatAgZGRkAgICAAACAhYUF3Nzc4O/vjw0bNqB+/frlGjPpxtPTE2fPnsWCBQswZcoU3L9/H/b29vD19cXatWtLfU/16tUxb948vP322+UcLelKoVA889yLfC8QScHHWxMREZFGHJIgIiIijZgwEBERkUZMGIiIiEgjJgxERESkERMGIiIi0ogJAxEREWnEhIGIiIg0YsJAVAEMGTIEvXv3Vr3u2LEjJk6cWO5x/PrrrxAE4blbCAuCgD179ki+5pw5c9C8eXOd4rp58yYEQcD58+d1ug4RvTgmDETPMGTIEAiCAEEQYGpqivr16yM8PByFhYVlfu9du3Zh3rx5kvpK+SFPRKQrbg1N9Bxdu3bFpk2bkJeXhx9++AGhoaGoXr06ZsyYUaJvfn4+TE1N9XJfOzs7vVyHiEhfWGEgeg65XA5HR0e4ubnh/fffR0BAAL7//nsA/w4jLFiwAM7OzvDy8gIA3LlzB/3794etrS3s7OzQq1cv3Lx5U3XNoqIiTJ48Gba2tqhZsyY++OADPL1D+9NDEnl5eZg+fTpcXFwgl8tRv359REZG4ubNm/D39wcA1KhRA4IgYMiQIQCePHk0IiICHh4eMDc3R7NmzfDNN9+o3eeHH35Aw4YNYW5uDn9/f7U4pZo+fToaNmwICwsLeHp6IiwsDAUFBSX6rV+/Hi4uLrCwsED//v2Rnp6udv7zzz+Hj48PzMzM4O3tzUdvE1UwTBiItGBubo78/HzV6+joaCQkJODgwYPYt2+f6pHD1tbW+O233/D777/DyspK9UhiAPjkk0+wefNmbNy4EUePHkVqaip279793Pu+++67+PLLL7FixQrExcVh/fr1sLKygouLC7799lsAQEJCAu7fv49PP/0UABAREYGtW7di3bp1uHz5MiZNmoTBgwfjyJEjAJ4kNn369EHPnj1x/vx5jBgxAh9++KHWfybW1tbYvHkz/vzzT3z66af47LPPsGzZMrU+165dw86dO7F3714cOHAA586dw5gxY1Tnt2/fjlmzZmHBggWIi4vDwoULERYWhi1btmgdDxGVEZGIShUSEiL26tVLFEVRVCqV4sGDB0W5XC5OnTpVdd7BwUHMy8tTvWfbtm2il5eXqFQqVW15eXmiubm5+NNPP4miKIpOTk7iokWLVOcLCgrEunXrqu4liqLYoUMHccKECaIoimJCQoIIQDx48GCpcf7yyy8iAPHRo0eqttzcXNHCwkI8duyYWt/hw4eLgwYNEkVRFGfMmCE2atRI7fz06dNLXOtpAMTdu3c/8/zixYtFX19f1evZs2eLJiYm4l9//aVq+/HHH0WZTCbev39fFEVRrFevnhgVFaV2nXnz5ol+fn6iKIpiYmKiCEA8d+7cM+9LRGWLcxiInmPfvn2wsrJCQUEBlEol3n77bcyZM0d1vmnTpmrzFi5cuIBr167B2tpa7Tq5ubm4fv060tPTcf/+fbRu3Vp1rlq1amjZsmWJYYli58+fh4mJCTp06CA57mvXriEnJwedO3dWa8/Pz8fLL78MAIiLi1OLAwD8/Pwk36PYV199hRUrVuD69evIyspCYWFhiccvu7q6ok6dOmr3USqVSEhIgLW1Na5fv47hw4dj5MiRqj6FhYWlPqKdiAyDCQPRc/j7+2Pt2rUwNTWFs7MzqlVT/ytjaWmp9jorKwu+vr7Yvn17iWvZ29u/UAzm5uZavycrKwsAsH//frUf1MCTeRn6Ehsbi+DgYMydOxeBgYGwsbHBjh078Mknn2gd62effVYigTExMdFbrESkGyYMRM9haWmJ+vXrS+7fokULfPXVV6hdu3aJ/2UXc3JywokTJ9C+fXsAT/4nfebMGbRo0aLU/k2bNoVSqcSRI0cQEBBQ4nxxhaOoqEjV1qhRI8jlcty+ffuZlQkfHx/VBM5ix48f1/wh/+PYsWNwc3PDRx99pGq7detWiX63b9/GvXv34OzsrLqPTCaDl5cXHBwc4OzsjBs3biA4OFir+xNR+eGkRyI9Cg4ORq1atdCrVy/89ttvSExMxK+//orx48fjr7/+AgBMmDAB//vf/7Bnzx7Ex8djzJgxz91Dwd3dHSEhIRg2bBj27NmjuubOnTsBAG5ubhAEAfv27cPDhw+RlZUFa2trTJ06FZMmTcKWLVtw/fp1nD17FitXrlRNJBw9ejSuXr2KadOmISEhAVFRUdi8ebNWn7dBgwa4ffs2duzYgevXr2PFihWlTuA0MzNDSEgILly4gN9++w3jx49H//794ejoCACYO3cuIiIisGLFCly5cgUXL17Epk2bsHTpUq3iIaKyw4SBSI8sLCwQExMDV1dX9OnTBz4+Phg+fDhyc3NVFYcpU6bgnXfeQUhICPz8/GBtbY0333zzudddu3Yt+vXrhzFjxsDb2xsjR45EdnY2AKBOnTqYO3cuPvzwQzg4OGDs2LEAgHnz5iEsLAwRERHw8fFB165dsX//fnh4eAB4Mq/g22+/xZ49e9CsWTOsW7cOCxcu1OrzvvHGG5g0aRLGjh2L5s2b49ixYwgLCyvRr379+ujTpw+6d++OLl264KWXXlJbNjlixAh8/vnn2LRpE5o2bYoOHTpg8+bNqliJyPAE8VkzrYiIiIj+wQoDERERacSEgYiIiDRiwkBEREQaMWEgIiIijZgwEBERkUZMGIiIiEgjJgxERESkERMGIiIi0ogJAxEREWnEhIGIiIg0YsJAREREGjFhICIiIo3+H6jgJsqss1EWAAAAAElFTkSuQmCC\n"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"acc = (all_preds == all_labels).mean()\nprint(f\"\\nOverall Test Accuracy: {acc*100:.4f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:36:14.256974Z","iopub.execute_input":"2025-07-31T20:36:14.257154Z","iopub.status.idle":"2025-07-31T20:36:14.261540Z","shell.execute_reply.started":"2025-07-31T20:36:14.257139Z","shell.execute_reply":"2025-07-31T20:36:14.260805Z"}},"outputs":[{"name":"stdout","text":"\nOverall Test Accuracy: 65.3153%\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\ntrain_dataset = ScanDataset(X_train, y_train)\ntest_dataset  = ScanDataset(X_test, y_test)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:36:14.262339Z","iopub.execute_input":"2025-07-31T20:36:14.262569Z","iopub.status.idle":"2025-07-31T20:36:14.288468Z","shell.execute_reply.started":"2025-07-31T20:36:14.262550Z","shell.execute_reply":"2025-07-31T20:36:14.287911Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'\\ntrain_dataset = ScanDataset(X_train, y_train)\\ntest_dataset  = ScanDataset(X_test, y_test)\\n'"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"'''\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2)\ntest_loader  = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=2)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:36:14.289224Z","iopub.execute_input":"2025-07-31T20:36:14.289492Z","iopub.status.idle":"2025-07-31T20:36:14.308871Z","shell.execute_reply.started":"2025-07-31T20:36:14.289470Z","shell.execute_reply":"2025-07-31T20:36:14.308168Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'\\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2)\\ntest_loader  = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=2)\\n'"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"'''\nclass TimeDistributedResNet(nn.Module):\n    def __init__(self, base_model):\n        super(TimeDistributedResNet, self).__init__()\n        self.base_model = base_model\n\n    def forward(self, x):\n        # x: (batch, time, 3, 128,128)\n        B, T, C, H, W = x.size()\n        x = x.view(B*T, C, H, W)\n        x = self.base_model(x)                   # (B*T, 2048)\n        x = x.view(B, T, -1)                     # (B, T, 2048)\n        return x\n\nclass ScanLSTMClassifier(nn.Module):\n    def __init__(self):\n        super(ScanLSTMClassifier, self).__init__()\n        base_resnet = models.resnet152(weights='IMAGENET1K_V1')\n        base_resnet.fc = nn.Identity()\n        self.resnet = TimeDistributedResNet(base_resnet)\n\n        for param in self.resnet.parameters():\n            param.requires_grad = False          # freeze ResNet\n\n        self.lstm = nn.LSTM(2048, 512, batch_first=True)\n        self.fc1 = nn.Linear(256, 128)\n        self.dropout = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(128, 1)\n\n    def forward(self, x):\n        x = self.resnet(x)                       # (B, T, 2048)\n        _, (hn, _) = self.lstm(x)                # hn: (1, B, 256)\n        x = hn.squeeze(0)                        # (B, 256)\n        x = F.relu(self.fc1(x))                  # (B, 128)\n        x = self.dropout(x)\n        x = torch.sigmoid(self.fc2(x)).squeeze(-1)  # (B,)\n        return x\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:36:14.309589Z","iopub.execute_input":"2025-07-31T20:36:14.309820Z","iopub.status.idle":"2025-07-31T20:36:14.327158Z","shell.execute_reply.started":"2025-07-31T20:36:14.309793Z","shell.execute_reply":"2025-07-31T20:36:14.326593Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"\"\\nclass TimeDistributedResNet(nn.Module):\\n    def __init__(self, base_model):\\n        super(TimeDistributedResNet, self).__init__()\\n        self.base_model = base_model\\n\\n    def forward(self, x):\\n        # x: (batch, time, 3, 128,128)\\n        B, T, C, H, W = x.size()\\n        x = x.view(B*T, C, H, W)\\n        x = self.base_model(x)                   # (B*T, 2048)\\n        x = x.view(B, T, -1)                     # (B, T, 2048)\\n        return x\\n\\nclass ScanLSTMClassifier(nn.Module):\\n    def __init__(self):\\n        super(ScanLSTMClassifier, self).__init__()\\n        base_resnet = models.resnet152(weights='IMAGENET1K_V1')\\n        base_resnet.fc = nn.Identity()\\n        self.resnet = TimeDistributedResNet(base_resnet)\\n\\n        for param in self.resnet.parameters():\\n            param.requires_grad = False          # freeze ResNet\\n\\n        self.lstm = nn.LSTM(2048, 512, batch_first=True)\\n        self.fc1 = nn.Linear(256, 128)\\n        self.dropout = nn.Dropout(0.3)\\n        self.fc2 = nn.Linear(128, 1)\\n\\n    def forward(self, x):\\n        x = self.resnet(x)                       # (B, T, 2048)\\n        _, (hn, _) = self.lstm(x)                # hn: (1, B, 256)\\n        x = hn.squeeze(0)                        # (B, 256)\\n        x = F.relu(self.fc1(x))                  # (B, 128)\\n        x = self.dropout(x)\\n        x = torch.sigmoid(self.fc2(x)).squeeze(-1)  # (B,)\\n        return x\\n\""},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"'''\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel1 = ScanLSTMClassifier().to(device)\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model1.parameters(), lr=1e-6)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:36:14.327752Z","iopub.execute_input":"2025-07-31T20:36:14.327953Z","iopub.status.idle":"2025-07-31T20:36:14.350249Z","shell.execute_reply.started":"2025-07-31T20:36:14.327937Z","shell.execute_reply":"2025-07-31T20:36:14.349526Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"\"\\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\nmodel1 = ScanLSTMClassifier().to(device)\\ncriterion = nn.BCELoss()\\noptimizer = torch.optim.Adam(model1.parameters(), lr=1e-6)\\n\""},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"'''\nclass ScanDataset(Dataset):\n    def __init__(self, scans, labels):\n        self.scans = scans\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.scans)\n\n    def __getitem__(self, idx):\n        x = self.scans[idx]  # shape: (100, 3, 128, 128)\n        y = self.labels[idx]\n        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:36:14.351054Z","iopub.execute_input":"2025-07-31T20:36:14.351299Z","iopub.status.idle":"2025-07-31T20:36:14.366556Z","shell.execute_reply.started":"2025-07-31T20:36:14.351277Z","shell.execute_reply":"2025-07-31T20:36:14.365878Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'\\nclass ScanDataset(Dataset):\\n    def __init__(self, scans, labels):\\n        self.scans = scans\\n        self.labels = labels\\n\\n    def __len__(self):\\n        return len(self.scans)\\n\\n    def __getitem__(self, idx):\\n        x = self.scans[idx]  # shape: (100, 3, 128, 128)\\n        y = self.labels[idx]\\n        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\\n'"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"'''\nN = 100  # number of entropy-based slices\ncrop_size = 128\n\nX, y = [], []\n\nfor label, path in tqdm(actual_paths):\n    scan = nib.load(path)\n    data = scan.get_fdata()\n    label_val = 0 if label == 'AD' else 1\n\n    slice_info = []\n\n    for axis in [0, 1, 2]:  \n        for i in range(data.shape[axis]):\n            # Extract 2D slice along the given axis \n            if axis == 0:\n                slice_ = data[i, :, :]\n            elif axis == 1:\n                slice_ = data[:, i, :]\n            else:\n                slice_ = data[:, :, i]\n\n            # Crop and skip empty slices\n            cropped = center_crop(slice_, crop_size=crop_size)\n            if cropped is None:\n                continue\n\n            # Compute entropy\n            ent = image_entropy(cropped)\n            slice_info.append((ent, cropped))\n\n    # Sort slices by entropy\n    slice_info.sort(reverse=True, key=lambda x: x[0])\n    top_slices = slice_info[:N]\n\n    if len(top_slices) < N:\n        print(f\"[!] Skipped subject: only {len(top_slices)} slices\")\n        continue\n\n    subject_slices = []\n\n    for _, slice_2d in top_slices:\n        # Grayscale slice is (128,128), add channel axis → (128,128,1)\n        slice_2d = slice_2d[..., np.newaxis]\n\n        # Repeat channel 3 times → (128,128,3)\n        slice_rgb = np.repeat(slice_2d, 3, axis=-1)\n\n        # Convert to channels-first → (3,128,128)\n        slice_rgb_chw = np.transpose(slice_rgb, (2,0,1))\n\n        subject_slices.append(slice_rgb_chw)\n\n    # Stack into (N,3,128,128)\n    subject_volume = np.stack(subject_slices, axis=0)\n\n    X.append(subject_volume)\n    y.append(label_val)\n\nX = np.stack(X)  # shape: (num_subjects, N, 128, 128, 1)\ny = np.array(y)  # shape: (num_subjects,)\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:36:14.367405Z","iopub.execute_input":"2025-07-31T20:36:14.367704Z","iopub.status.idle":"2025-07-31T20:36:14.392285Z","shell.execute_reply.started":"2025-07-31T20:36:14.367679Z","shell.execute_reply":"2025-07-31T20:36:14.391653Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'\\nN = 100  # number of entropy-based slices\\ncrop_size = 128\\n\\nX, y = [], []\\n\\nfor label, path in tqdm(actual_paths):\\n    scan = nib.load(path)\\n    data = scan.get_fdata()\\n    label_val = 0 if label == \\'AD\\' else 1\\n\\n    slice_info = []\\n\\n    for axis in [0, 1, 2]:  \\n        for i in range(data.shape[axis]):\\n            # Extract 2D slice along the given axis \\n            if axis == 0:\\n                slice_ = data[i, :, :]\\n            elif axis == 1:\\n                slice_ = data[:, i, :]\\n            else:\\n                slice_ = data[:, :, i]\\n\\n            # Crop and skip empty slices\\n            cropped = center_crop(slice_, crop_size=crop_size)\\n            if cropped is None:\\n                continue\\n\\n            # Compute entropy\\n            ent = image_entropy(cropped)\\n            slice_info.append((ent, cropped))\\n\\n    # Sort slices by entropy\\n    slice_info.sort(reverse=True, key=lambda x: x[0])\\n    top_slices = slice_info[:N]\\n\\n    if len(top_slices) < N:\\n        print(f\"[!] Skipped subject: only {len(top_slices)} slices\")\\n        continue\\n\\n    subject_slices = []\\n\\n    for _, slice_2d in top_slices:\\n        # Grayscale slice is (128,128), add channel axis → (128,128,1)\\n        slice_2d = slice_2d[..., np.newaxis]\\n\\n        # Repeat channel 3 times → (128,128,3)\\n        slice_rgb = np.repeat(slice_2d, 3, axis=-1)\\n\\n        # Convert to channels-first → (3,128,128)\\n        slice_rgb_chw = np.transpose(slice_rgb, (2,0,1))\\n\\n        subject_slices.append(slice_rgb_chw)\\n\\n    # Stack into (N,3,128,128)\\n    subject_volume = np.stack(subject_slices, axis=0)\\n\\n    X.append(subject_volume)\\n    y.append(label_val)\\n\\nX = np.stack(X)  # shape: (num_subjects, N, 128, 128, 1)\\ny = np.array(y)  # shape: (num_subjects,)\\n\\n'"},"metadata":{}}],"execution_count":24}]}